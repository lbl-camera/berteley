{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic_Modeling_Pipeline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdA/qwGIH3yrzZBMhNwlWb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dani-lbnl/mudit/blob/main/Topic_Modeling_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages"
      ],
      "metadata": {
        "id": "Xdm-UxhVsgxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U kaleido\n",
        "!pip install bertopic\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMYhyoX28wzd",
        "outputId": "2fc12c49-06dc-42d2-f00f-7200842b0756"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.7/dist-packages (0.9.4)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (4.64.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.7/dist-packages (from bertopic) (0.8.28)\n",
            "Requirement already satisfied: pyyaml<6.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (5.4.1)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (5.5.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.3.5)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (2.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.21.5)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->bertopic) (0.29.28)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->bertopic) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->bertopic) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.5.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.10.0+cu111)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.96)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.11.1+cu111)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.0.49)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (4.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.8)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.0->bertopic) (0.51.2)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.6)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsMDdg9VrsAR",
        "outputId": "ff6fb977-b0ad-412d-b05e-22e058eee666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# from top2vec import Top2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import time\n",
        "from google.colab import drive\n",
        "import os\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import kaleido\n",
        "\n",
        "multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "class LemmaTokenizer:\n",
        "  def __init__(self):\n",
        "    self.wnl = WordNetLemmatizer()\n",
        "  def __call__(self, doc):\n",
        "    return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5AfUGmuo9fX",
        "outputId": "4617ec85-4adc-4f68-a11d-78c1912b6617"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "class LemmaTokenizer:\n",
        "  def __init__(self):\n",
        "    self.wnl = WordNetLemmatizer()\n",
        "  def __call__(self, doc):\n",
        "    # return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "    return [self.wnl.t.lemma_ for t in nlp(doc)]"
      ],
      "metadata": {
        "id": "5PZSg91kHSZK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define path of files"
      ],
      "metadata": {
        "id": "jmy68rUEsla3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O26_s1bZtcsL",
        "outputId": "96a4fc3e-f8ee-42ef-829f-4cac6138118b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read all files\n",
        "path = '/content/drive/MyDrive/NLP/ALS Spreadsheets/'\n",
        "files = os.listdir(path)\n",
        "files.sort()\n",
        "files\n",
        "# beamline 2.4 bad ..removed; also one more check which one\n",
        "# 3.2.1 bad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku3tdw1F5Dyl",
        "outputId": "9d598d3a-8476-448d-dafd-d6e714350349"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beamline_1.4.xls',\n",
              " 'Beamline_10.0.1.xls',\n",
              " 'Beamline_10.3.1.xls',\n",
              " 'Beamline_10.3.2.xls',\n",
              " 'Beamline_11.0.1.xls',\n",
              " 'Beamline_11.0.2.xls',\n",
              " 'Beamline_11.3.1.xls',\n",
              " 'Beamline_11.3.2.xls',\n",
              " 'Beamline_12.0.1.1.xls',\n",
              " 'Beamline_12.0.1.2.xls',\n",
              " 'Beamline_12.0.1.4.xls',\n",
              " 'Beamline_12.0.2.xls',\n",
              " 'Beamline_12.2.1.xls',\n",
              " 'Beamline_12.2.2.xls',\n",
              " 'Beamline_12.3.1.xls',\n",
              " 'Beamline_12.3.2.xls',\n",
              " 'Beamline_2.1.xls',\n",
              " 'Beamline_3.1.1.xls',\n",
              " 'Beamline_3.3.2.xls',\n",
              " 'Beamline_4.0.2.xls',\n",
              " 'Beamline_4.0.3.1.xls',\n",
              " 'Beamline_4.0.3.2.xls',\n",
              " 'Beamline_4.2.2.xls',\n",
              " 'Beamline_5.0.1.xls',\n",
              " 'Beamline_5.0.2.xls',\n",
              " 'Beamline_5.0.3.xls',\n",
              " 'Beamline_5.3.1.xls',\n",
              " 'Beamline_5.3.2.1.xls',\n",
              " 'Beamline_5.3.2.2.xls',\n",
              " 'Beamline_5.4.xls',\n",
              " 'Beamline_6.0.1.xls',\n",
              " 'Beamline_6.0.2.xls',\n",
              " 'Beamline_6.1.2.xls',\n",
              " 'Beamline_6.3.1.xls',\n",
              " 'Beamline_6.3.2.xls',\n",
              " 'Beamline_7.0.1.xls',\n",
              " 'Beamline_7.0.2.xls',\n",
              " 'Beamline_7.3.1.xls',\n",
              " 'Beamline_7.3.3.xls',\n",
              " 'Beamline_8.0.1.xls',\n",
              " 'Beamline_8.2.1.xls',\n",
              " 'Beamline_8.2.2.xls',\n",
              " 'Beamline_8.3.1.xls',\n",
              " 'Beamline_8.3.2.xls',\n",
              " 'Beamline_9.0.xls',\n",
              " 'Beamline_9.3.1.xls',\n",
              " 'beamline_4.0.3.xls']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files[-5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FqYt10GYysze",
        "outputId": "b131e26a-2a1b-4a8e-be19-0ccd8e6cd119"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Beamline_8.3.1.xls'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "DjbU3FESipc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_input(df):\n",
        "\n",
        "  # for title\n",
        "  indices = df['Title'].isna()\n",
        "  df.loc[indices,'Title'] = \"\"\n",
        "\n",
        "  # for abstract\n",
        "  indices = df['Abstract'].isna()\n",
        "  df.loc[indices,'Abstract'] = \"\"\n",
        "\n",
        "  # combined - title + abstract\n",
        "  df['Combined'] = df['Title'] + \" \" + df['Abstract']\n",
        "  combined = list(df['Combined'])\n",
        "\n",
        "  # remove elements with blank\n",
        "  while \" \" in combined:\n",
        "    combined.remove(\" \")\n",
        "\n",
        "  # remove patterns\n",
        "  pattern = r'<inf>|</inf>|<sup>|</sup>|inf|/inf'\n",
        "  comb_clean = []\n",
        "  for l in combined:\n",
        "    mod_string = re.sub(pattern, ' ', l )\n",
        "    comb_clean.append(mod_string)\n",
        "\n",
        "\n",
        "  return comb_clean"
      ],
      "metadata": {
        "id": "ON8pUtt264NQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_spacy([\"my names is Mudit\",\"Hers name is Kritika\"])"
      ],
      "metadata": {
        "id": "S1CdQf7eWxtb",
        "outputId": "17ed5033-68c0-4166-fdb3-95a2bb72969f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-PRON- name be Mudit', 'her name be Kritika']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemma_spacy(df_combined):\n",
        "\n",
        "  new_df= []\n",
        "  for doc in df_combined:\n",
        "    filt_combined = []\n",
        "    for word in nlp(doc):\n",
        "      filt_combined.append(word.lemma_)\n",
        "\n",
        "    new_df.append(\" \".join(filt_combined))\n",
        "\n",
        "  return new_df\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "yHA8zNZpWZmm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop(df_combined):\n",
        "\n",
        "  # remove stopwords\n",
        "\n",
        "  new_df= []\n",
        "  for doc in df_combined:\n",
        "    filt_combined = []\n",
        "    for word in word_tokenize(doc):\n",
        "\n",
        "      if word.lower() not in stopwords.words('english'):\n",
        "        # print(word)\n",
        "        if word.lower() == \"perovskites\":\n",
        "          filt_combined.append(\"perovskite\")\n",
        "        else:\n",
        "          filt_combined.append(word)\n",
        "    new_df.append(\" \".join(filt_combined))\n",
        "\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "dKhO1Drz7nE9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def berttopic_basic(df):\n",
        "\n",
        "  topic_model = BERTopic()\n",
        "  topics, probs = topic_model.fit_transform(df)\n",
        "\n",
        "  # added lemmatization\n",
        "  # update using lemmatization\n",
        "\n",
        "  tf_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "  # vectorizer_model = CountVectorizer(stop_words=\"English\", ngram_range=(1, 5))\n",
        "  topic_model.update_topics(df, topics, vectorizer_model=tf_vectorizer)\n",
        "\n",
        "  # topic_info_df = pd.DataFrame(topic_model.get_topic_info())\n",
        "\n",
        "  return topic_model"
      ],
      "metadata": {
        "id": "yO6a7-6L9UWm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os \n",
        "\n",
        "def create_path_if_not_exists(datapath):\n",
        "    '''Create the new file if not exists and save the data'''\n",
        "\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath) "
      ],
      "metadata": {
        "id": "U4JEJrdy_ykb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## example"
      ],
      "metadata": {
        "id": "jzLjxK7Zzu0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_table(path + \"Beamline_7.3.3.xls\", on_bad_lines='skip')\n",
        "\n",
        "# combine abstract and title\n",
        "input_data = create_input(df)\n",
        "\n",
        "# remove stopwords\n",
        "filtered_input = remove_stop(input_data)"
      ],
      "metadata": {
        "id": "SmPBjsDuztrD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using BERTopic"
      ],
      "metadata": {
        "id": "T4Kqcax2isN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xn6F9B_9FECQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "iter_version = \"BERTopic_v2/\"\n",
        "\n",
        "\n",
        "for i in range(len(files)):\n",
        "  print(\"File:\",str(i+1) + ' ' + files[i])\n",
        "  beam_name = files[i].split('xls')[0][:-1]\n",
        "\n",
        "  df = pd.read_table(path + files[i], on_bad_lines='skip')\n",
        "\n",
        "\n",
        "  # combine abstract and title\n",
        "  input_data = create_input(df)\n",
        "\n",
        "  # remove stopwords\n",
        "  filtered_input = remove_stop(input_data)\n",
        "\n",
        "  # run topic modeling algorithm\n",
        "  model = berttopic_basic(filtered_input)\n",
        "\n",
        "  base_path = '/content/drive/MyDrive/NLP/'\n",
        "  model_path = base_path + iter_version + beam_name + \"/\"\n",
        "  create_path_if_not_exists(model_path)\n",
        "\n",
        "  # save model as pickle file\n",
        "  file_path = model_path + \"model\" + beam_name + \".pkl\"\n",
        "  joblib.dump(model, file_path) \n",
        "\n",
        "  # visualize as html\n",
        "  fig = model.visualize_barchart(top_n_topics = len(model.topics))\n",
        "  fig_name = model_path + \"bar_chart\" + beam_name  +\".html\"\n",
        "  fig_name_png = model_path + \"bar_chart\" + beam_name  +\".png\"\n",
        "\n",
        "  fig.write_html(fig_name)\n",
        "  fig.write_image(fig_name_png)\n",
        "\n",
        "\n",
        "  # save as excel\n",
        "  # df_info = pd.DataFrame(model.get_topic_info())\n",
        "  excel_name = model_path + \"Topic_Results.xlsx\"\n",
        "  # df_info.to_excel(excel_name,sheet_name=\"topic_info\")\n",
        "\n",
        "  df_topics = pd.DataFrame(model.topics)\n",
        "  df_topics.to_excel(excel_name,sheet_name=\"topic_words\")\n",
        "\n",
        "  print(\"File: {} done \".format(i+1) )\n",
        "\n",
        "\n",
        "print('Total time taken (mins): ', int((time.time()-start_time)/60))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C24ayaLF5NZs",
        "outputId": "69feed47-6168-409c-d5bc-435b9f74eed2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: 1 Beamline_1.4.xls\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: 1 done \n",
            "File: 2 Beamline_10.0.1.xls\n",
            "File: 2 done \n",
            "File: 3 Beamline_10.3.1.xls\n",
            "File: 3 done \n",
            "File: 4 Beamline_10.3.2.xls\n",
            "File: 4 done \n",
            "File: 5 Beamline_11.0.1.xls\n",
            "File: 5 done \n",
            "File: 6 Beamline_11.0.2.xls\n",
            "File: 6 done \n",
            "File: 7 Beamline_11.3.1.xls\n",
            "File: 7 done \n",
            "File: 8 Beamline_11.3.2.xls\n",
            "File: 8 done \n",
            "File: 9 Beamline_12.0.1.1.xls\n",
            "File: 9 done \n",
            "File: 10 Beamline_12.0.1.2.xls\n",
            "File: 10 done \n",
            "File: 11 Beamline_12.0.1.4.xls\n",
            "File: 11 done \n",
            "File: 12 Beamline_12.0.2.xls\n",
            "File: 12 done \n",
            "File: 13 Beamline_12.2.1.xls\n",
            "File: 13 done \n",
            "File: 14 Beamline_12.2.2.xls\n",
            "File: 14 done \n",
            "File: 15 Beamline_12.3.1.xls\n",
            "File: 15 done \n",
            "File: 16 Beamline_12.3.2.xls\n",
            "File: 16 done \n",
            "File: 17 Beamline_2.1.xls\n",
            "File: 17 done \n",
            "File: 18 Beamline_3.1.1.xls\n",
            "File: 18 done \n",
            "File: 19 Beamline_3.3.2.xls\n",
            "File: 19 done \n",
            "File: 20 Beamline_4.0.2.xls\n",
            "File: 20 done \n",
            "File: 21 Beamline_4.0.3.1.xls\n",
            "File: 21 done \n",
            "File: 22 Beamline_4.0.3.2.xls\n",
            "File: 22 done \n",
            "File: 23 Beamline_4.2.2.xls\n",
            "File: 23 done \n",
            "File: 24 Beamline_5.0.1.xls\n",
            "File: 24 done \n",
            "File: 25 Beamline_5.0.2.xls\n",
            "File: 25 done \n",
            "File: 26 Beamline_5.0.3.xls\n",
            "File: 26 done \n",
            "File: 27 Beamline_5.3.1.xls\n",
            "File: 27 done \n",
            "File: 28 Beamline_5.3.2.1.xls\n",
            "File: 28 done \n",
            "File: 29 Beamline_5.3.2.2.xls\n",
            "File: 29 done \n",
            "File: 30 Beamline_5.4.xls\n",
            "File: 30 done \n",
            "File: 31 Beamline_6.0.1.xls\n",
            "File: 31 done \n",
            "File: 32 Beamline_6.0.2.xls\n",
            "File: 32 done \n",
            "File: 33 Beamline_6.1.2.xls\n",
            "File: 33 done \n",
            "File: 34 Beamline_6.3.1.xls\n",
            "File: 34 done \n",
            "File: 35 Beamline_6.3.2.xls\n",
            "File: 35 done \n",
            "File: 36 Beamline_7.0.1.xls\n",
            "File: 36 done \n",
            "File: 37 Beamline_7.0.2.xls\n",
            "File: 37 done \n",
            "File: 38 Beamline_7.3.1.xls\n",
            "File: 38 done \n",
            "File: 39 Beamline_7.3.3.xls\n",
            "File: 39 done \n",
            "File: 40 Beamline_8.0.1.xls\n",
            "File: 40 done \n",
            "File: 41 Beamline_8.2.1.xls\n",
            "File: 41 done \n",
            "File: 42 Beamline_8.2.2.xls\n",
            "File: 42 done \n",
            "File: 43 Beamline_8.3.1.xls\n",
            "File: 43 done \n",
            "File: 44 Beamline_8.3.2.xls\n",
            "File: 44 done \n",
            "File: 45 Beamline_9.0.xls\n",
            "File: 45 done \n",
            "File: 46 Beamline_9.3.1.xls\n",
            "File: 46 done \n",
            "File: 47 beamline_4.0.3.xls\n",
            "File: 47 done \n",
            "Total time taken (mins):  25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9te8PAvs5RRx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using SPECTER"
      ],
      "metadata": {
        "id": "yaXPQ6DNzXTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punct(df):\n",
        "\n",
        "  # remove punctuation\n",
        "  text_col = [''.join(letter for letter in word if letter not in string.punctuation) for word in df]\n",
        "\n",
        "  return text_col\n"
      ],
      "metadata": {
        "id": "Mo0sv6XLb_ke"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def berttopic_specter(df):\n",
        "\n",
        "  sentence_model = SentenceTransformer('allenai-specter')\n",
        "  topic_model_specter = BERTopic(embedding_model=sentence_model)\n",
        "\n",
        "  topics, probs = topic_model_specter.fit_transform(df)\n",
        "\n",
        "  # added lemmatization\n",
        "  # update using lemmatization\n",
        "\n",
        "  tf_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "  # vectorizer_model = CountVectorizer(stop_words=\"English\", ngram_range=(1, 5))\n",
        "  topic_model_specter.update_topics(df, topics, vectorizer_model=tf_vectorizer)\n",
        "\n",
        "  # topic_info_df = pd.DataFrame(topic_model.get_topic_info())\n",
        "\n",
        "  return topic_model_specter"
      ],
      "metadata": {
        "id": "3iRBL5hmVSZE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "iter_version = \"BERTopic_SPECTER_v1/\"\n",
        "\n",
        "\n",
        "for i in range(len(files)):\n",
        "  print(\"File:\",str(i+1) + ' ' + files[i])\n",
        "  beam_name = files[i].split('xls')[0][:-1]\n",
        "\n",
        "  df = pd.read_table(path + files[i], on_bad_lines='skip')\n",
        "\n",
        "\n",
        "  # combine abstract and title\n",
        "  input_data = create_input(df)\n",
        "\n",
        "  # remove stopwords\n",
        "  filtered_input = remove_stop(input_data)\n",
        "\n",
        "  # remove stopwords\n",
        "  filtered_input_2 = remove_punct(filtered_input)\n",
        "\n",
        "  # run topic modeling algorithm\n",
        "  model = berttopic_specter(filtered_input_2)\n",
        "\n",
        "  base_path = '/content/drive/MyDrive/NLP/'\n",
        "  model_path = base_path + iter_version + beam_name + \"/\"\n",
        "  create_path_if_not_exists(model_path)\n",
        "\n",
        "  # save model as pickle file\n",
        "  file_path = model_path + \"model\" + beam_name + \".pkl\"\n",
        "  joblib.dump(model, file_path) \n",
        "\n",
        "  # visualize as html\n",
        "  fig = model.visualize_barchart(top_n_topics = len(model.topics))\n",
        "  fig_name = model_path + \"bar_chart\" + beam_name  +\".html\"\n",
        "  fig_name_png = model_path + \"bar_chart\" + beam_name  +\".png\"\n",
        "\n",
        "  fig.write_html(fig_name)\n",
        "  fig.write_image(fig_name_png)\n",
        "\n",
        "\n",
        "  # save as excel\n",
        "  # df_info = pd.DataFrame(model.get_topic_info())\n",
        "  excel_name = model_path + \"Topic_Results.xlsx\"\n",
        "  # df_info.to_excel(excel_name,sheet_name=\"topic_info\")\n",
        "\n",
        "  df_topics = pd.DataFrame(model.topics)\n",
        "  df_topics.to_excel(excel_name,sheet_name=\"topic_words\")\n",
        "\n",
        "  print(\"File: {} done \".format(i+1) )\n",
        "\n",
        "\n",
        "print('Total time taken (mins): ', int((time.time()-start_time)/60))\n"
      ],
      "metadata": {
        "id": "tMpV2YiwUk8d",
        "outputId": "52d3bf1b-2923-4aa7-bfda-242f4d17fd9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: 1 Beamline_1.4.xls\n",
            "File: 1 done \n",
            "File: 2 Beamline_10.0.1.xls\n",
            "File: 2 done \n",
            "File: 3 Beamline_10.3.1.xls\n",
            "File: 3 done \n",
            "File: 4 Beamline_10.3.2.xls\n",
            "File: 4 done \n",
            "File: 5 Beamline_11.0.1.xls\n",
            "File: 5 done \n",
            "File: 6 Beamline_11.0.2.xls\n",
            "File: 6 done \n",
            "File: 7 Beamline_11.3.1.xls\n",
            "File: 7 done \n",
            "File: 8 Beamline_11.3.2.xls\n",
            "File: 8 done \n",
            "File: 9 Beamline_12.0.1.1.xls\n",
            "File: 9 done \n",
            "File: 10 Beamline_12.0.1.2.xls\n",
            "File: 10 done \n",
            "File: 11 Beamline_12.0.1.4.xls\n",
            "File: 11 done \n",
            "File: 12 Beamline_12.0.2.xls\n",
            "File: 12 done \n",
            "File: 13 Beamline_12.2.1.xls\n",
            "File: 13 done \n",
            "File: 14 Beamline_12.2.2.xls\n",
            "File: 14 done \n",
            "File: 15 Beamline_12.3.1.xls\n",
            "File: 15 done \n",
            "File: 16 Beamline_12.3.2.xls\n",
            "File: 16 done \n",
            "File: 17 Beamline_2.1.xls\n",
            "File: 17 done \n",
            "File: 18 Beamline_3.1.1.xls\n",
            "File: 18 done \n",
            "File: 19 Beamline_3.3.2.xls\n",
            "File: 19 done \n",
            "File: 20 Beamline_4.0.2.xls\n",
            "File: 20 done \n",
            "File: 21 Beamline_4.0.3.1.xls\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-6705bbfbce8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# visualize as html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_barchart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_n_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0mfig_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"bar_chart\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeam_name\u001b[0m  \u001b[0;34m+\u001b[0m\u001b[0;34m\".html\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mfig_name_png\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"bar_chart\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeam_name\u001b[0m  \u001b[0;34m+\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mvisualize_barchart\u001b[0;34m(self, topics, top_n_topics, n_words, width, height)\u001b[0m\n\u001b[1;32m   1232\u001b[0m                                            \u001b[0mn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                                            \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m                                            height=height)\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     def save(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bertopic/plotting/_barchart.py\u001b[0m in \u001b[0;36mvisualize_barchart\u001b[0;34m(topic_model, topics, top_n_topics, n_words, width, height)\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0mhorizontal_spacing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                         \u001b[0mvertical_spacing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.4\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                         subplot_titles=subplot_titles)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Add barchart for each topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/plotly/subplots.py\u001b[0m in \u001b[0;36mmake_subplots\u001b[0;34m(rows, cols, shared_xaxes, shared_yaxes, start_cell, print_grid, horizontal_spacing, vertical_spacing, subplot_titles, column_widths, row_heights, specs, insets, column_titles, row_titles, x_title, y_title, figure, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0mThe\u001b[0m \u001b[0;34m'rows'\u001b[0m \u001b[0margument\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmake_suplots\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0man\u001b[0m \u001b[0mint\u001b[0m \u001b[0mgreater\u001b[0m \u001b[0mthan\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     Received value of type {typ}: {val}\"\"\".format(\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             )\n\u001b[1;32m    368\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: \nThe 'rows' argument to make_suplots must be an int greater than 0.\n    Received value of type <class 'int'>: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gLAA5PfbV-JB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic_Modeling_Pipeline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJ2F7mhZaBn9IbhIA23mCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dani-lbnl/mudit/blob/main/Topic_Modeling_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages"
      ],
      "metadata": {
        "id": "Xdm-UxhVsgxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMYhyoX28wzd",
        "outputId": "d6709d3a-5549-401f-b537-74611b28ef60"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.9.4-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 33.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 57 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml<6.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (3.13)\n",
            "Collecting sentence-transformers>=0.4.1\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (4.63.0)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (5.5.0)\n",
            "Collecting hdbscan>=0.8.27\n",
            "  Downloading hdbscan-0.8.28.tar.gz (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 35.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0\n",
            "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.21.5)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->bertopic) (0.29.28)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->bertopic) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->bertopic) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.11.1+cu111)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
            "Collecting pyyaml<6.0\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (4.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.7)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.0->bertopic) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.34.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-linux_x86_64.whl size=2330835 sha256=44b5132fa09887928f270730f3e49ffd8fc47fa7be823838378e91e1f7e0cf96\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/7a/5e/259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=27ec3515fdb51b5c7e69648b977096c56ee75be2b39c3f5742f5c21c3bfe9d77\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=d32a55fced0278f1ed1670affe28b7b69eb3356492240dc443ab30e00d660b19\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=d2840712b4ef64fd491b100088c8dcaf5fce1346b3287ea4a1807f68d940d818\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, pynndescent, umap-learn, sentence-transformers, hdbscan, bertopic\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed bertopic-0.9.4 hdbscan-0.8.28 huggingface-hub-0.5.1 pynndescent-0.5.6 pyyaml-5.4.1 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.18.0 umap-learn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsMDdg9VrsAR",
        "outputId": "ef546aa2-f50a-40eb-c3b7-8538cd8db1c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# from top2vec import Top2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import time\n",
        "from google.colab import drive\n",
        "import os\n",
        "from bertopic import BERTopic\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data from Google Drive"
      ],
      "metadata": {
        "id": "jmy68rUEsla3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O26_s1bZtcsL",
        "outputId": "84c588a8-eeb2-4ba7-db2d-9c4010caaed7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read all files\n",
        "path = '/content/drive/MyDrive/NLP/ALS Spreadsheets/'\n",
        "files = os.listdir(path)\n",
        "files.sort()\n",
        "files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku3tdw1F5Dyl",
        "outputId": "097c8cd1-949f-4d4d-f9da-b3b0016a03c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beamline_1.4.xls',\n",
              " 'Beamline_10.0.1.xls',\n",
              " 'Beamline_10.3.1.xls',\n",
              " 'Beamline_10.3.2.xls',\n",
              " 'Beamline_11.0.1.xls',\n",
              " 'Beamline_11.0.2.xls',\n",
              " 'Beamline_11.3.1.xls',\n",
              " 'Beamline_11.3.2.xls',\n",
              " 'Beamline_12.0.1.1.xls',\n",
              " 'Beamline_12.0.1.2.xls',\n",
              " 'Beamline_12.0.1.4.xls',\n",
              " 'Beamline_12.0.2.xls',\n",
              " 'Beamline_12.2.1.xls',\n",
              " 'Beamline_12.2.2.xls',\n",
              " 'Beamline_12.3.1.xls',\n",
              " 'Beamline_12.3.2.xls',\n",
              " 'Beamline_2.1.xls',\n",
              " 'Beamline_2.4.xls',\n",
              " 'Beamline_3.1.1.xls',\n",
              " 'Beamline_3.2.1.xls',\n",
              " 'Beamline_3.3.2.xls',\n",
              " 'Beamline_4.0.2.xls',\n",
              " 'Beamline_4.0.3.1.xls',\n",
              " 'Beamline_4.0.3.2.xls',\n",
              " 'Beamline_4.2.2.xls',\n",
              " 'Beamline_5.0.1.xls',\n",
              " 'Beamline_5.0.2.xls',\n",
              " 'Beamline_5.0.3.xls',\n",
              " 'Beamline_5.3.1.xls',\n",
              " 'Beamline_5.3.2.1.xls',\n",
              " 'Beamline_5.3.2.2.xls',\n",
              " 'Beamline_5.4.xls',\n",
              " 'Beamline_6.0.1.xls',\n",
              " 'Beamline_6.0.2.xls',\n",
              " 'Beamline_6.1.2.xls',\n",
              " 'Beamline_6.3.1.xls',\n",
              " 'Beamline_6.3.2.xls',\n",
              " 'Beamline_7.0.1.xls',\n",
              " 'Beamline_7.0.2.xls',\n",
              " 'Beamline_7.3.1.xls',\n",
              " 'Beamline_7.3.3.xls',\n",
              " 'Beamline_8.0.1.xls',\n",
              " 'Beamline_8.2.1.xls',\n",
              " 'Beamline_8.2.2.xls',\n",
              " 'Beamline_8.3.1.xls',\n",
              " 'Beamline_8.3.2.xls',\n",
              " 'Beamline_9.0.xls',\n",
              " 'Beamline_9.3.1.xls',\n",
              " 'beamline_4.0.3.xls']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_input(df):\n",
        "\n",
        "  # for title\n",
        "  indices = df['Title'].isna()\n",
        "  df.loc[indices,'Title'] = \"\"\n",
        "\n",
        "  # for abstract\n",
        "  indices = df['Abstract'].isna()\n",
        "  df.loc[indices,'Abstract'] = \"\"\n",
        "\n",
        "  # combined - title + abstract\n",
        "  df['Combined'] = df['Title'] + \" \" + df['Abstract']\n",
        "  combined = list(df['Combined'])\n",
        "  print(\"Length (before removing blanks):\",len(combined))\n",
        "\n",
        "  # remove elements with blank\n",
        "  combined.remove(\" \")\n",
        "  print(\"Length (after removing blanks):\",len(combined))\n",
        "\n",
        "  return combined\n",
        "\n"
      ],
      "metadata": {
        "id": "ON8pUtt264NQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop(df_combined):\n",
        "\n",
        "  # remove stopwords\n",
        "\n",
        "  new_df= []\n",
        "  for doc in df_combined:\n",
        "    filt_combined = []\n",
        "    for word in word_tokenize(doc):\n",
        "      # print(word)\n",
        "      if word not in stopwords.words('english'):\n",
        "        # print(word)\n",
        "        filt_combined.append(word)\n",
        "    new_df.append(\" \".join(filt_combined))\n",
        "\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "dKhO1Drz7nE9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def berttopic_basic(df):\n",
        "\n",
        "  topic_model = BERTopic()\n",
        "  topics, probs = topic_model.fit_transform(df)\n",
        "\n",
        "  # create df of topic info \n",
        "  "
      ],
      "metadata": {
        "id": "yO6a7-6L9UWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CSSUCKw_4gw",
        "outputId": "649328e1-2e1d-44f8-eb6d-17a572d1d0fd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os \n",
        "\n",
        "def create_path_if_not_exists(datapath):\n",
        "    '''Create the new file if not exists and save the data'''\n",
        "\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath) \n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "  # Create directory\n",
        "  model_path = 'model/model2/XGBoost/version_2'\n",
        "  create_path_if_not_exists(model_path)\n",
        "  \n",
        "  # Save file\n",
        "  joblib.dump(model, model_path)"
      ],
      "metadata": {
        "id": "U4JEJrdy_ykb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(files)):\n",
        "  print(\"File:\",str(i+1) + ' ' + files[i])\n",
        "  df = pd.read_table(path + files[i], on_bad_lines='skip')\n",
        "\n",
        "  # combine abstract and title\n",
        "  input_data = create_input(df)\n",
        "\n",
        "  # remove stopwords\n",
        "  filtered_input = remove_stop(input_data)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C24ayaLF5NZs",
        "outputId": "3239a4b7-114d-46d2-9670-7b79a0ef8731"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: 1 Beamline_1.4.xls\n",
            "File: 2 Beamline_10.0.1.xls\n",
            "File: 3 Beamline_10.3.1.xls\n",
            "File: 4 Beamline_10.3.2.xls\n",
            "File: 5 Beamline_11.0.1.xls\n",
            "File: 6 Beamline_11.0.2.xls\n",
            "File: 7 Beamline_11.3.1.xls\n",
            "File: 8 Beamline_11.3.2.xls\n",
            "File: 9 Beamline_12.0.1.1.xls\n",
            "File: 10 Beamline_12.0.1.2.xls\n",
            "File: 11 Beamline_12.0.1.4.xls\n",
            "File: 12 Beamline_12.0.2.xls\n",
            "File: 13 Beamline_12.2.1.xls\n",
            "File: 14 Beamline_12.2.2.xls\n",
            "File: 15 Beamline_12.3.1.xls\n",
            "File: 16 Beamline_12.3.2.xls\n",
            "File: 17 Beamline_2.1.xls\n",
            "File: 18 Beamline_2.4.xls\n",
            "File: 19 Beamline_3.1.1.xls\n",
            "File: 20 Beamline_3.2.1.xls\n",
            "File: 21 Beamline_3.3.2.xls\n",
            "File: 22 Beamline_4.0.2.xls\n",
            "File: 23 Beamline_4.0.3.1.xls\n",
            "File: 24 Beamline_4.0.3.2.xls\n",
            "File: 25 Beamline_4.2.2.xls\n",
            "File: 26 Beamline_5.0.1.xls\n",
            "File: 27 Beamline_5.0.2.xls\n",
            "File: 28 Beamline_5.0.3.xls\n",
            "File: 29 Beamline_5.3.1.xls\n",
            "File: 30 Beamline_5.3.2.1.xls\n",
            "File: 31 Beamline_5.3.2.2.xls\n",
            "File: 32 Beamline_5.4.xls\n",
            "File: 33 Beamline_6.0.1.xls\n",
            "File: 34 Beamline_6.0.2.xls\n",
            "File: 35 Beamline_6.1.2.xls\n",
            "File: 36 Beamline_6.3.1.xls\n",
            "File: 37 Beamline_6.3.2.xls\n",
            "File: 38 Beamline_7.0.1.xls\n",
            "File: 39 Beamline_7.0.2.xls\n",
            "File: 40 Beamline_7.3.1.xls\n",
            "File: 41 Beamline_7.3.3.xls\n",
            "File: 42 Beamline_8.0.1.xls\n",
            "File: 43 Beamline_8.2.1.xls\n",
            "File: 44 Beamline_8.2.2.xls\n",
            "File: 45 Beamline_8.3.1.xls\n",
            "File: 46 Beamline_8.3.2.xls\n",
            "File: 47 Beamline_9.0.xls\n",
            "File: 48 Beamline_9.3.1.xls\n",
            "File: 49 beamline_4.0.3.xls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9te8PAvs5RRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yaXPQ6DNzXTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
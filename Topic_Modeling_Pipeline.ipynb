{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic_Modeling_Pipeline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFEtixSYveN/357zDBHpde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dani-lbnl/mudit/blob/main/Topic_Modeling_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages"
      ],
      "metadata": {
        "id": "Xdm-UxhVsgxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMYhyoX28wzd",
        "outputId": "ecfff340-5419-463c-87e4-c9916489b56c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.7/dist-packages (0.9.4)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (4.64.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.0.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (2.2.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.7/dist-packages (from bertopic) (0.8.28)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.21.5)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (5.5.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.3.5)\n",
            "Requirement already satisfied: pyyaml<6.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (5.4.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->bertopic) (0.29.28)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->bertopic) (1.4.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->bertopic) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.18.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.10.0+cu111)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.2.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (4.11.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.0.49)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.8)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.6)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.0->bertopic) (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.34.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsMDdg9VrsAR",
        "outputId": "94706a9d-ff41-4d0e-8d2e-cd6e84f9869e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# from top2vec import Top2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import time\n",
        "from google.colab import drive\n",
        "import os\n",
        "from bertopic import BERTopic\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "class LemmaTokenizer:\n",
        "  def __init__(self):\n",
        "    self.wnl = WordNetLemmatizer()\n",
        "  def __call__(self, doc):\n",
        "    return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5AfUGmuo9fX",
        "outputId": "af51208d-9a9d-41a8-91a5-676ef49c5c47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bQCqnNBGGDF",
        "outputId": "353ec47d-80ea-41fa-d944-5086deba872d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kaleido"
      ],
      "metadata": {
        "id": "5PZSg91kHSZK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define path of files"
      ],
      "metadata": {
        "id": "jmy68rUEsla3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O26_s1bZtcsL",
        "outputId": "7742752a-6674-459f-cec8-66c836fdbbd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read all files\n",
        "path = '/content/drive/MyDrive/NLP/ALS Spreadsheets/'\n",
        "files = os.listdir(path)\n",
        "files.sort()\n",
        "files\n",
        "# beamline 2.4 bad ..removed; also one more check which one\n",
        "# 3.2.1 bad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku3tdw1F5Dyl",
        "outputId": "9477bbe6-5a87-48d0-d63a-5725b8148afa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beamline_1.4.xls',\n",
              " 'Beamline_10.0.1.xls',\n",
              " 'Beamline_10.3.1.xls',\n",
              " 'Beamline_10.3.2.xls',\n",
              " 'Beamline_11.0.1.xls',\n",
              " 'Beamline_11.0.2.xls',\n",
              " 'Beamline_11.3.1.xls',\n",
              " 'Beamline_11.3.2.xls',\n",
              " 'Beamline_12.0.1.1.xls',\n",
              " 'Beamline_12.0.1.2.xls',\n",
              " 'Beamline_12.0.1.4.xls',\n",
              " 'Beamline_12.0.2.xls',\n",
              " 'Beamline_12.2.1.xls',\n",
              " 'Beamline_12.2.2.xls',\n",
              " 'Beamline_12.3.1.xls',\n",
              " 'Beamline_12.3.2.xls',\n",
              " 'Beamline_2.1.xls',\n",
              " 'Beamline_3.1.1.xls',\n",
              " 'Beamline_3.3.2.xls',\n",
              " 'Beamline_4.0.2.xls',\n",
              " 'Beamline_4.0.3.1.xls',\n",
              " 'Beamline_4.0.3.2.xls',\n",
              " 'Beamline_4.2.2.xls',\n",
              " 'Beamline_5.0.1.xls',\n",
              " 'Beamline_5.0.2.xls',\n",
              " 'Beamline_5.0.3.xls',\n",
              " 'Beamline_5.3.1.xls',\n",
              " 'Beamline_5.3.2.1.xls',\n",
              " 'Beamline_5.3.2.2.xls',\n",
              " 'Beamline_5.4.xls',\n",
              " 'Beamline_6.0.1.xls',\n",
              " 'Beamline_6.0.2.xls',\n",
              " 'Beamline_6.1.2.xls',\n",
              " 'Beamline_6.3.1.xls',\n",
              " 'Beamline_6.3.2.xls',\n",
              " 'Beamline_7.0.1.xls',\n",
              " 'Beamline_7.0.2.xls',\n",
              " 'Beamline_7.3.1.xls',\n",
              " 'Beamline_7.3.3.xls',\n",
              " 'Beamline_8.0.1.xls',\n",
              " 'Beamline_8.2.1.xls',\n",
              " 'Beamline_8.2.2.xls',\n",
              " 'Beamline_8.3.1.xls',\n",
              " 'Beamline_8.3.2.xls',\n",
              " 'Beamline_9.0.xls',\n",
              " 'Beamline_9.3.1.xls',\n",
              " 'beamline_4.0.3.xls']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files[-5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FqYt10GYysze",
        "outputId": "f019a156-7175-4621-8173-9fcb5af35535"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Beamline_8.3.1.xls'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "DjbU3FESipc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_input(df):\n",
        "\n",
        "  # for title\n",
        "  indices = df['Title'].isna()\n",
        "  df.loc[indices,'Title'] = \"\"\n",
        "\n",
        "  # for abstract\n",
        "  indices = df['Abstract'].isna()\n",
        "  df.loc[indices,'Abstract'] = \"\"\n",
        "\n",
        "  # combined - title + abstract\n",
        "  df['Combined'] = df['Title'] + \" \" + df['Abstract']\n",
        "  combined = list(df['Combined'])\n",
        "\n",
        "  # remove elements with blank\n",
        "  while \" \" in combined:\n",
        "    combined.remove(\" \")\n",
        "\n",
        "  # remove patterns\n",
        "  pattern = r'<inf>|</inf>|<sup>|</sup>|inf|/inf'\n",
        "  comb_clean = []\n",
        "  for l in combined:\n",
        "    mod_string = re.sub(pattern, ' ', l )\n",
        "    comb_clean.append(mod_string)\n",
        "\n",
        "\n",
        "  return comb_clean"
      ],
      "metadata": {
        "id": "ON8pUtt264NQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop(df_combined):\n",
        "\n",
        "  # remove stopwords\n",
        "\n",
        "  new_df= []\n",
        "  for doc in df_combined:\n",
        "    filt_combined = []\n",
        "    for word in word_tokenize(doc):\n",
        "\n",
        "      if word.lower() not in stopwords.words('english'):\n",
        "        # print(word)\n",
        "        if word.lower() == \"perovskites\":\n",
        "          filt_combined.append(\"perovskite\")\n",
        "        else:\n",
        "          filt_combined.append(word)\n",
        "    new_df.append(\" \".join(filt_combined))\n",
        "\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "dKhO1Drz7nE9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def berttopic_basic(df):\n",
        "\n",
        "  topic_model = BERTopic()\n",
        "  topics, probs = topic_model.fit_transform(df)\n",
        "\n",
        "  # added lemmatization\n",
        "  # update using lemmatization\n",
        "\n",
        "  tf_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "  # vectorizer_model = CountVectorizer(stop_words=\"English\", ngram_range=(1, 5))\n",
        "  topic_model.update_topics(df, topics, vectorizer_model=tf_vectorizer)\n",
        "\n",
        "  # topic_info_df = pd.DataFrame(topic_model.get_topic_info())\n",
        "\n",
        "  return topic_model"
      ],
      "metadata": {
        "id": "yO6a7-6L9UWm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os \n",
        "\n",
        "def create_path_if_not_exists(datapath):\n",
        "    '''Create the new file if not exists and save the data'''\n",
        "\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath) "
      ],
      "metadata": {
        "id": "U4JEJrdy_ykb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## example"
      ],
      "metadata": {
        "id": "jzLjxK7Zzu0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqrdzBfRAQrV",
        "outputId": "c665874f-1707-4cf3-df88-25eab89cd76a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 563 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=827db70dcdc4f6e3d7e78defb9425729cd1c2386e655925f4b135cbae712c59d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wp0xn9h0/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "doc = nlp(u\"perovskites\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token.lemma, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3SED3SCAPiZ",
        "outputId": "2c3ca17e-6c47-455c-aff0-fb9e428f747c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perovskites 12575679595001236785 perovskite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        " \n",
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "print(\"Perovskite :\", lemmatizer.lemmatize(\"Perovskite\"))\n",
        "print(\"Perovskites :\", lemmatizer.lemmatize(\"perovskites\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAdSar7F_vD-",
        "outputId": "49481886-6cf2-4775-8c4f-3b799a456cf7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perovskite : Perovskite\n",
            "Perovskites : perovskites\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_table(path + \"Beamline_7.3.3.xls\", on_bad_lines='skip')\n",
        "\n",
        "# combine abstract and title\n",
        "input_data = create_input(df)\n",
        "\n",
        "# remove stopwords\n",
        "filtered_input = remove_stop(input_data)"
      ],
      "metadata": {
        "id": "SmPBjsDuztrD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Run"
      ],
      "metadata": {
        "id": "T4Kqcax2isN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_version = \"BERTopic_v2/\""
      ],
      "metadata": {
        "id": "xn6F9B_9FECQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "\n",
        "for i in range(len(files)):\n",
        "  print(\"File:\",str(i+1) + ' ' + files[i])\n",
        "  beam_name = files[i].split('xls')[0][:-1]\n",
        "\n",
        "  df = pd.read_table(path + files[i], on_bad_lines='skip')\n",
        "\n",
        "\n",
        "  # combine abstract and title\n",
        "  input_data = create_input(df)\n",
        "\n",
        "  # remove stopwords\n",
        "  filtered_input = remove_stop(input_data)\n",
        "\n",
        "  # run topic modeling algorithm\n",
        "  model = berttopic_basic(filtered_input)\n",
        "\n",
        "  base_path = '/content/drive/MyDrive/NLP/'\n",
        "  model_path = base_path + iter_version + beam_name + \"/\"\n",
        "  create_path_if_not_exists(model_path)\n",
        "\n",
        "  # save model as pickle file\n",
        "  file_path = model_path + \"model\" + beam_name + \".pkl\"\n",
        "  joblib.dump(model, file_path) \n",
        "\n",
        "  # visualize as html\n",
        "  fig = model.visualize_barchart(top_n_topics = len(model.topics))\n",
        "  fig_name = model_path + \"bar_chart\" + beam_name  +\".html\"\n",
        "  fig_name_png = model_path + \"bar_chart\" + beam_name  +\".png\"\n",
        "\n",
        "  fig.write_html(fig_name)\n",
        "  fig.write_image(fig_name_png)\n",
        "\n",
        "\n",
        "  # save as excel\n",
        "  # df_info = pd.DataFrame(model.get_topic_info())\n",
        "  excel_name = model_path + \"Topic_Results.xlsx\"\n",
        "  # df_info.to_excel(excel_name,sheet_name=\"topic_info\")\n",
        "\n",
        "  df_topics = pd.DataFrame(model.topics)\n",
        "  df_topics.to_excel(excel_name,sheet_name=\"topic_words\")\n",
        "\n",
        "  print(\"File: {} done \".format(i+1) )\n",
        "\n",
        "\n",
        "print('Total time taken (mins): ', int((time.time()-start_time)/60))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C24ayaLF5NZs",
        "outputId": "69feed47-6168-409c-d5bc-435b9f74eed2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: 1 Beamline_1.4.xls\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: 1 done \n",
            "File: 2 Beamline_10.0.1.xls\n",
            "File: 2 done \n",
            "File: 3 Beamline_10.3.1.xls\n",
            "File: 3 done \n",
            "File: 4 Beamline_10.3.2.xls\n",
            "File: 4 done \n",
            "File: 5 Beamline_11.0.1.xls\n",
            "File: 5 done \n",
            "File: 6 Beamline_11.0.2.xls\n",
            "File: 6 done \n",
            "File: 7 Beamline_11.3.1.xls\n",
            "File: 7 done \n",
            "File: 8 Beamline_11.3.2.xls\n",
            "File: 8 done \n",
            "File: 9 Beamline_12.0.1.1.xls\n",
            "File: 9 done \n",
            "File: 10 Beamline_12.0.1.2.xls\n",
            "File: 10 done \n",
            "File: 11 Beamline_12.0.1.4.xls\n",
            "File: 11 done \n",
            "File: 12 Beamline_12.0.2.xls\n",
            "File: 12 done \n",
            "File: 13 Beamline_12.2.1.xls\n",
            "File: 13 done \n",
            "File: 14 Beamline_12.2.2.xls\n",
            "File: 14 done \n",
            "File: 15 Beamline_12.3.1.xls\n",
            "File: 15 done \n",
            "File: 16 Beamline_12.3.2.xls\n",
            "File: 16 done \n",
            "File: 17 Beamline_2.1.xls\n",
            "File: 17 done \n",
            "File: 18 Beamline_3.1.1.xls\n",
            "File: 18 done \n",
            "File: 19 Beamline_3.3.2.xls\n",
            "File: 19 done \n",
            "File: 20 Beamline_4.0.2.xls\n",
            "File: 20 done \n",
            "File: 21 Beamline_4.0.3.1.xls\n",
            "File: 21 done \n",
            "File: 22 Beamline_4.0.3.2.xls\n",
            "File: 22 done \n",
            "File: 23 Beamline_4.2.2.xls\n",
            "File: 23 done \n",
            "File: 24 Beamline_5.0.1.xls\n",
            "File: 24 done \n",
            "File: 25 Beamline_5.0.2.xls\n",
            "File: 25 done \n",
            "File: 26 Beamline_5.0.3.xls\n",
            "File: 26 done \n",
            "File: 27 Beamline_5.3.1.xls\n",
            "File: 27 done \n",
            "File: 28 Beamline_5.3.2.1.xls\n",
            "File: 28 done \n",
            "File: 29 Beamline_5.3.2.2.xls\n",
            "File: 29 done \n",
            "File: 30 Beamline_5.4.xls\n",
            "File: 30 done \n",
            "File: 31 Beamline_6.0.1.xls\n",
            "File: 31 done \n",
            "File: 32 Beamline_6.0.2.xls\n",
            "File: 32 done \n",
            "File: 33 Beamline_6.1.2.xls\n",
            "File: 33 done \n",
            "File: 34 Beamline_6.3.1.xls\n",
            "File: 34 done \n",
            "File: 35 Beamline_6.3.2.xls\n",
            "File: 35 done \n",
            "File: 36 Beamline_7.0.1.xls\n",
            "File: 36 done \n",
            "File: 37 Beamline_7.0.2.xls\n",
            "File: 37 done \n",
            "File: 38 Beamline_7.3.1.xls\n",
            "File: 38 done \n",
            "File: 39 Beamline_7.3.3.xls\n",
            "File: 39 done \n",
            "File: 40 Beamline_8.0.1.xls\n",
            "File: 40 done \n",
            "File: 41 Beamline_8.2.1.xls\n",
            "File: 41 done \n",
            "File: 42 Beamline_8.2.2.xls\n",
            "File: 42 done \n",
            "File: 43 Beamline_8.3.1.xls\n",
            "File: 43 done \n",
            "File: 44 Beamline_8.3.2.xls\n",
            "File: 44 done \n",
            "File: 45 Beamline_9.0.xls\n",
            "File: 45 done \n",
            "File: 46 Beamline_9.3.1.xls\n",
            "File: 46 done \n",
            "File: 47 beamline_4.0.3.xls\n",
            "File: 47 done \n",
            "Total time taken (mins):  25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9te8PAvs5RRx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yaXPQ6DNzXTI"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}
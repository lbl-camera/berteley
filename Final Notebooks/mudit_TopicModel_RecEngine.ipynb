{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModel_RecEngine.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericchagnon15/BERTeley/blob/main/Final%20Notebooks/TopicModel_RecEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install and imports"
      ],
      "metadata": {
        "id": "OX_a3v9tWeSh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26N2QnWaUQI7"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install -U kaleido\n",
        "!pip install octis\n",
        "!pip install bertopic\n",
        "# !python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following cell needs to be run twice"
      ],
      "metadata": {
        "id": "iZlLobIlhrdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell may need to be run twice\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import time\n",
        "from google.colab import drive\n",
        "import os\n",
        "import joblib\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
        "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "from scipy import spatial\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import kaleido\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "multiprocessing.cpu_count()"
      ],
      "metadata": {
        "id": "XNNNPo-sWl-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## helper functions"
      ],
      "metadata": {
        "id": "4AbiC8mxWsqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_path_if_not_exists(datapath):\n",
        "  '''\n",
        "  If the given path does not exist, a directory is created at the location specified in the path\n",
        "\n",
        "  Input: \n",
        "  datapath: string of the path you want to create\n",
        "\n",
        "  Output:\n",
        "  none\n",
        "  '''\n",
        "\n",
        "  if not os.path.exists(datapath):\n",
        "      os.makedirs(datapath) \n",
        "\n",
        "#\n",
        "# Removes to punctuation from the string in each row\n",
        "# df: \n",
        "#\n",
        "\n",
        "def remove_punct_df(row_string):\n",
        "\n",
        "  '''\n",
        "  Tokenizes each word in the dataframe and checks to see if the word is a punctuation character\n",
        "  If there is any punctuation it is removed from the string\n",
        "\n",
        "  Input: \n",
        "  row_string: single row from a dataframe that contains the text in the corpus, a single string\n",
        "\n",
        "  Output:\n",
        "  a string with no punctuation\n",
        "  '''\n",
        "\n",
        "  filt_combined = []\n",
        "  for word in word_tokenize(row_string):\n",
        "\n",
        "    if word.lower() not in string.punctuation:\n",
        "      \n",
        "      \n",
        "      filt_combined.append(word.lower())\n",
        "  filtered_ip= \" \".join(filt_combined)\n",
        "\n",
        "  return filtered_ip\n",
        "\n",
        "def format_dataframe(df):\n",
        "\n",
        "  '''\n",
        "  The ALS data has some extra fields that are not needed, we only want to keep\n",
        "  Authors, Pub Year, Research Area, Pub TYpe\n",
        "  Create a new field 'Combined' which is the concatenation of the Title and Abstract\n",
        "  Rows with NA or missing values are removed\n",
        "\n",
        "  Input: \n",
        "  df: pandas dataframe containing the raw data downloaded from the ALS database\n",
        "\n",
        "  Output:\n",
        "  newly formated dataframe with proper fields\n",
        "  '''\n",
        "\n",
        "  # NA in title converted to blanks\n",
        "  indices = df['Title'].isna()\n",
        "  df.loc[indices,'Title'] = \"\"\n",
        "\n",
        "  # NA in abstract converted to blanks\n",
        "  indices = df['Abstract'].isna()\n",
        "  df.loc[indices,'Abstract'] = \"\"\n",
        "\n",
        "  # combined - title + abstract\n",
        "  df['Combined'] = df['Title'] + \" \" + df['Abstract']\n",
        "\n",
        "  # remove blanks\n",
        "  df = df[df['Combined']!=\" \"]\n",
        "\n",
        "  # keep only selected cols\n",
        "  df_sel = df[['Authors','Pub Year','Research Area','Combined',\"Pub TYpe\"]]\n",
        "  df_sel= df_sel.rename(columns={'Pub Year':'pub_year',\"Research Area\":\"research_area\",\"Authors\":\"authors\",\"Combined\":\"combined\",\"Pub TYpe\":\"pub_type\"})\n",
        "\n",
        "  combined = list(df_sel['combined'])\n",
        "\n",
        "  # remove patterns\n",
        "  pattern = r'<inf>|</inf>|<sup>|</sup>|inf|/inf'\n",
        "  comb_clean = []\n",
        "\n",
        "  for l in combined:\n",
        "    mod_string = re.sub(pattern, '', l )\n",
        "    comb_clean.append(mod_string)\n",
        "\n",
        "  # merge back to df\n",
        "  df_sel['combined'] = comb_clean\n",
        "\n",
        "  # filter spurious years\n",
        "  df_sel = df_sel[df_sel['pub_year']!='12.0.1.2']\n",
        "\n",
        "  # convert years to int\n",
        "  df_sel['pub_year'] = df_sel['pub_year'].astype(str).replace('\\.0', '', regex=True).astype(int)\n",
        "  # if year is 201, that is mistyped fom 2001\n",
        "  df_sel[df_sel['pub_year']==201]['pub_year'] = 2001\n",
        "\n",
        "\n",
        "  return df_sel\n",
        "\n",
        "\n",
        "\n",
        "def lemma_spacy(row_string):\n",
        "\n",
        "  '''\n",
        "  This function utilizes the lemmatizer in the spacy package to lemmatize all the words in the list\n",
        "\n",
        "  Input: \n",
        "  row_string: single row from a dataframe that contains the text in the corpus, a single string\n",
        "\n",
        "  Output:\n",
        "  a string with lemmatized words\n",
        "  '''\n",
        "\n",
        "  filt_combined = []\n",
        "  for word in nlp(row_string):\n",
        "    if word.lemma_ != '-PRON-' :\n",
        "      filt_combined.append(word.lemma_)\n",
        "\n",
        "  new_df = \" \".join(filt_combined)\n",
        "\n",
        "  return new_df  \n",
        "\n",
        "\n",
        "def remove_stop_df(row_string, allow_abbrev = True):\n",
        "  \n",
        "  '''\n",
        "  This function utilizes the stopwords in the nltk package to remove the stopwords from the string. \n",
        "  Some further steps were taken for the specific ALS use case including removal of words that do not contain a letter\n",
        "  and words that are in the supplemental stopword list created after looking at initial outputs from early iterations of the topic model\n",
        "\n",
        "  Input: \n",
        "  row_string: single row from a dataframe that contains the text in the corpus, a single string\n",
        "\n",
        "  Output:\n",
        "  a string with removed stopwords\n",
        "  '''\n",
        "\n",
        "  filt_combined = []\n",
        "  als_stopwords = ['use', 'award', 'prize', 'academy', 'Thailand', 'high', 'scientist', 'medal', 'science',\n",
        "                   'fellow', 'career', 'presentation', 'early', 'shirley', 'david', 'achievement']\n",
        "\n",
        "\n",
        "  for word in word_tokenize(row_string):\n",
        "    contains_letter = re.search('[a-zA-Z]', word) != None\n",
        "    #if (word.lower() not in stopwords.words('english')) and (contains_letter) and (word.lower not in als_stopwords):\n",
        "    if word.lower() not in stopwords.words('english'): #check nltk stopwords\n",
        "      if re.search('[a-zA-Z]', word.lower()) != None: #check there is a letter\n",
        "        if word.lower() not in als_stopwords: #check ALS stopwords\n",
        "\n",
        "          if not allow_abbrev:\n",
        "            if len(word.lower()) > 2:\n",
        "              if word.lower() == \"perovskites\":\n",
        "                filt_combined.append(\"perovskite\")\n",
        "              else:\n",
        "                filt_combined.append(word)\n",
        "\n",
        "          else:\n",
        "\n",
        "            if word.lower() == \"perovskites\":\n",
        "              filt_combined.append(\"perovskite\")\n",
        "            else:\n",
        "              filt_combined.append(word)\n",
        "\n",
        "    filtered_ip= \" \".join(filt_combined)\n",
        "  return filtered_ip\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_metrics(topic_model, texts):\n",
        "  '''\n",
        "  calculates evaluation metrics for a given topic model\n",
        "  in this case it calculates Topic Coherence and Topic Diversity\n",
        "  Input\n",
        "  topic_model: the BERTopic object the metrics will be calculated on\n",
        "  texts: the list of documents the BERTopic model was trained on, this is a list of strings\n",
        "\n",
        "  Output\n",
        "  a list containing the metrics in the form [coherence_score, diversity_score]\n",
        "  '''\n",
        "\n",
        "\n",
        "  #octis requires the texts input be in the form of a list of list of strings\n",
        "  octis_texts = [sentence.split() for sentence in list_text] \n",
        "\n",
        "  \n",
        "  npmi = Coherence(texts = octis_texts, topk = 10, measure = 'c_npmi')\n",
        "  topic_diversity = TopicDiversity(topk=10)\n",
        "\n",
        "  # reformat the output of BERTopic to the proper format\n",
        "  # {topics: [[topic, words, for, topic1], [topic, words, for, topic2], [etc, etc, etc]]}\n",
        "  all_words = [word for words in octis_texts for word in words]\n",
        "\n",
        "  #check to see if the topic word has a space, if it does then this is a bigram model\n",
        "  if ' ' in topic_model.get_topic(0)[:1][0][0]:\n",
        "    #print(\"Bigram\")\n",
        "    bertopic_topics = [\n",
        "      [\n",
        "      vals[0] if (vals[0].split()[0] in all_words or vals[0].split()[1] in all_words) else all_words[0] \n",
        "      for vals in topic_model.get_topic(i)[:10]\n",
        "      ]\n",
        "    for i in range(len(set(topics)) - 1)\n",
        "    ]\n",
        "\n",
        "    output_tm = {\"topics\": bertopic_topics}\n",
        "\n",
        "    coherence_score = _calculate_coherence(topic_model, texts)\n",
        "\n",
        "  #unigram\n",
        "  else:\n",
        "    #print(\"Unigram\")\n",
        "    bertopic_topics = [\n",
        "                    [\n",
        "                        vals[0] if vals[0] in all_words else all_words[0]\n",
        "                        for vals in topic_model.get_topic(i)[:10]\n",
        "                    ]\n",
        "                    for i in range(len(set(topics)) - 1)\n",
        "                ]\n",
        "  \n",
        "    output_tm = {\"topics\": bertopic_topics}\n",
        "\n",
        "    coherence_score = npmi.score(output_tm)\n",
        "\n",
        "\n",
        "  diversity_score = topic_diversity.score(output_tm)\n",
        "\n",
        "  return {\"Coherence\": coherence_score, \"Diversity\": diversity_score}\n",
        "\n",
        "  #return [coherence_score, diversity_score]\n",
        "\n",
        "\n",
        "def _calculate_coherence(topic_model, texts):\n",
        "\n",
        "  '''\n",
        "  Calculates the coherence metric for bigrams\n",
        "\n",
        "\n",
        "  Input\n",
        "  topic_model: the BERTopic model object\n",
        "  texts: list of documents used to train the BERTopic model, this is a list of strings\n",
        "\n",
        "  Output\n",
        "  returns the coherence score which ranges from [-1, 1]\n",
        "  '''\n",
        "\n",
        "  docs = texts\n",
        "\n",
        "  # Preprocess Documents\n",
        "  documents = pd.DataFrame({\"Document\": docs,\n",
        "                            \"ID\": range(len(docs)),\n",
        "                            \"Topic\": topics})\n",
        "  documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "  cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
        "\n",
        "  # Extract vectorizer and analyzer from BERTopic\n",
        "  vectorizer = topic_model.vectorizer_model\n",
        "  analyzer = vectorizer.build_analyzer()\n",
        "\n",
        "  # Extract features for Topic Coherence evaluation\n",
        "  words = vectorizer.get_feature_names()\n",
        "  tokens = [analyzer(doc) for doc in cleaned_docs]\n",
        "  dictionary = corpora.Dictionary(tokens)\n",
        "  corpus = [dictionary.doc2bow(token) for token in tokens]\n",
        "  topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
        "                for topic in range(len(set(topics))-1)]\n",
        "\n",
        "  # Evaluate\n",
        "  coherence_model = CoherenceModel(topics=topic_words, \n",
        "                                  texts=tokens, \n",
        "                                  corpus=corpus,\n",
        "                                  dictionary=dictionary, \n",
        "                                  coherence='c_npmi')\n",
        "  coherence = coherence_model.get_coherence()\n",
        "  return coherence\n",
        "\n",
        "\n",
        "def flatten(t):\n",
        "  '''\n",
        "  \n",
        "\n",
        "  Input: \n",
        "  t: \n",
        "\n",
        "  Output:\n",
        "  \n",
        "  '''\n",
        "  \n",
        "  return [item for sublist in t for item in sublist]\n",
        "\n",
        "\n",
        "def get_authors(input_data,rep_docs):\n",
        "\n",
        "  '''\n",
        "  extract authors from the input data for the given documents(rep_docs)\n",
        "\n",
        "  Input: \n",
        "  input_data: data containing information about the documents\n",
        "  rep_docs: documents that you want to find the authors for\n",
        "\n",
        "\n",
        "  Output: list of all unique authors that authored the documents in rep_docs\n",
        "  \n",
        "  '''\n",
        "  # print(\"***Entered this function***\")\n",
        "  tt = input_data['combined'].to_list()\n",
        "\n",
        "  auth_list= []\n",
        "  for d in rep_docs:\n",
        "    ind = tt.index(d)\n",
        "    auth_str = input_data.authors.to_list()[ind]\n",
        "    auth_el = auth_str.split(\" ,\")\n",
        "    auth_list.append(auth_el)\n",
        "\n",
        "  auth_list = flatten(auth_list)\n",
        "  if '' in auth_list:\n",
        "    auth_list.remove('')\n",
        "\n",
        "  # unduplicate repeating authors \n",
        "\n",
        "  final_auth_list = list(set(auth_list))\n",
        "  if '' in final_auth_list:\n",
        "    final_auth_list.remove('')\n",
        "\n",
        "  return final_auth_list\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "# for all topics, get respective docs and then find respective authors  \n",
        "# topics: \n",
        "# input_data:  \n",
        "#\n",
        "def author_all_topics(topics,input_data):\n",
        "\n",
        "  dict_df = {}\n",
        "\n",
        "  # get a dict of all documents for each doc \n",
        "  topic_docs = {topic: [] for topic in set(topics)}\n",
        "  for topic, doc in zip(topics, input_data['combined']):\n",
        "      topic_docs[topic].append(doc)\n",
        "\n",
        "\n",
        "  for i in range(len(set(topics))-1):\n",
        "    author_list = get_authors(input_data,topic_docs[i])\n",
        "\n",
        "    dict_df[i] = author_list\n",
        "\n",
        "  # create df with topics and authors\n",
        "  author_topics = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in dict_df.items() ]))\n",
        "\n",
        "  return author_topics,dict_df\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "# find closest author to each doc by using each doc as a search term\n",
        "# relevant_docs: \n",
        "#\n",
        "\n",
        "def closest_author_docs(relevant_docs):\n",
        "\n",
        "  auth_list = []\n",
        "\n",
        "  for doc in relevant_docs:\n",
        "    similar_topics, similarity = topic_model.find_topics(doc, top_n=5)\n",
        "    auth_list.append(dict_authors[similar_topics[0]])\n",
        "\n",
        "  final_auth_list = list(set(flatten(auth_list)))\n",
        "\n",
        "\n",
        "  return final_auth_list\n",
        "\n"
      ],
      "metadata": {
        "id": "WKEYleUdWu4Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_input_new_springer(df):\n",
        "    \n",
        "\n",
        "  # for title\n",
        "  indices = df['Title'].isna()\n",
        "  df.loc[indices,'Title'] = \"\"\n",
        "\n",
        "  # for abstract\n",
        "  indices = df['Abstract'].isna()\n",
        "  df.loc[indices,'Abstract'] = \"\"\n",
        "\n",
        "  # combined - title + abstract\n",
        "  df['combined'] = df['Title'] + \" \" + df['Abstract']\n",
        "\n",
        "  # remove blanks\n",
        "  df = df[df['combined']!=\" \"]\n",
        "\n",
        "  #keep only selected cols\n",
        "  # df_sel = df[['Authors','Pub Year','Research Area','Combined',\"Pub TYpe\"]]\n",
        "  # df_sel= df_sel.rename(columns={'Pub Year':'pub_year',\"Research Area\":\"research_area\",\"Authors\":\"authors\",\"Combined\":\"combined\",\"Pub TYpe\":\"pub_type\"})\n",
        "\n",
        "  combined = list(df['combined'])\n",
        "\n",
        "  # remove patterns\n",
        "  pattern = r'<inf>|</inf>|<sup>|</sup>|inf|/inf'\n",
        "  comb_clean = []\n",
        "  for l in combined:\n",
        "    mod_string = re.sub(pattern, '', l )\n",
        "    comb_clean.append(mod_string)\n",
        "\n",
        "  # merge back to df\n",
        "  df['combined'] = comb_clean\n",
        "\n",
        "  # filter spurtious yeats\n",
        "  # df = df[df['pub_year']!='12.0.1.2']\n",
        "\n",
        "  # convert years to int\n",
        "  # df_sel['pub_year'] = df_sel['pub_year'].astype(str)\n",
        "  # df['pub_year'] = df_sel['pub_year'].astype(str).replace('\\.0', '', regex=True).astype(int)\n",
        "  # if year is 201, that is mistyped fom 2001\n",
        "  # df_sel[df_sel['pub_year']==201]['pub_year'] = 2001\n",
        "\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "4x1YFnk6HUxF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## path of files "
      ],
      "metadata": {
        "id": "Pom9Fj3nWxWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# location of csv files\n",
        "data_path = '/content/drive/MyDrive/NLP/Data/ALS Spreadsheets/'\n",
        "files = os.listdir(data_path)\n",
        "files.sort()\n",
        "#len(files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg_FCIh-Wz4A",
        "outputId": "963c950c-2786-4fee-c03e-c85056cfc6b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## topic model"
      ],
      "metadata": {
        "id": "zEkbronsW3NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# which model to use - \"default\" [\"mini-LM-L6-v2\"] or \"specter\" \n",
        "model_use = \"default\"\n",
        "\n",
        "# select which beamline to run\n",
        "beamline = \"5.0.3\"\n",
        "\n",
        "# ngram range - (1,1); (2,2); (1,2)\n",
        "# unigram is (1,1), bigram is (2,2)\n",
        "n_gram = (1,1)\n",
        "\n",
        "n_gram_type = \"\"\n",
        "if n_gram == (1,1): \n",
        "  n_gram_type = \"Unigram\"\n",
        "elif n_gram == (2,2):\n",
        "   n_gram_type = \"Bigram\"\n",
        "\n",
        "# name of folder the results will be saved in\n",
        "iter_version = \"test_results/\"\n",
        "\n",
        "# location of the folder for the results\n",
        "base_path = '/content/drive/MyDrive/NLP/Results/'"
      ],
      "metadata": {
        "id": "LweRYa_UXVFy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import csv of beamline\n",
        "beam_name = \"Beamline_\" + beamline + \".xls\"\n",
        "df = pd.read_table(data_path + beam_name, on_bad_lines='skip')\n",
        "\n",
        "\n",
        "# preprocess the data, maybe put these all in a function?\n",
        "\n",
        "input_data = format_dataframe(df)\n",
        "#input_data = df\n",
        "\n",
        "# remove stopwords from the combined column\n",
        "input_data['combined'] = input_data['combined'].apply(remove_stop_df, allow_abbrev = False)\n",
        "\n",
        "# lemmatize words in the combined column\n",
        "input_data['combined'] = input_data['combined'].apply(lemma_spacy)\n",
        "\n",
        "# remove punct from the combined column\n",
        "input_data['combined'] = input_data['combined'].apply(remove_punct_df)"
      ],
      "metadata": {
        "id": "R6tduJ_9zYQH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "n_topics = 10\n",
        "\n",
        "# train topic model\n",
        "if model_use == \"specter\":\n",
        "  sentence_model = SentenceTransformer('allenai-specter')\n",
        "  topic_model = BERTopic(embedding_model=sentence_model,verbose=True,nr_topics = n_topics,n_gram_range=n_gram) \n",
        "else: \n",
        "  topic_model = BERTopic(verbose=True,nr_topics = n_topics,n_gram_range=n_gram) # uses default bertopic model - \"all-miniLM-L6_v2\"\n",
        "\n",
        "# convert the column of combined text into a list of strings\n",
        "list_text = input_data['combined'].to_list()\n",
        "\n",
        "topics, probs = topic_model.fit_transform(list_text)\n",
        "\n",
        "print('Total training time taken (mins): ', float((time.time()-start_time)/60))\n",
        "\n",
        "\n",
        "###################################################\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "## dynamic topic modeling - topics over time \n",
        "years = input_data['pub_year'].to_list() # save years from the dataframe\n",
        "topics_over_time = topic_model.topics_over_time(list_text, topics, years) # train dynamic topic model\n",
        "\n",
        "\n",
        "print('Dynamic Topic modeling total time taken (mins): ', float((time.time()-start_time)/60))"
      ],
      "metadata": {
        "id": "VNhJApo9fyJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving results"
      ],
      "metadata": {
        "id": "_zl-SVqrQoCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "# create beamline folder in the given path to which model files will be saved \n",
        "model_path = base_path + iter_version + beamline + \"_\" + model_use + \"_\" + n_gram_type + \"/\"\n",
        "create_path_if_not_exists(model_path)\n",
        "\n",
        "# save topic model as pickle file\n",
        "def save_model(filename):\n",
        "  model_path = base_path + iter_version + beamline + \"_\" + model_use + \"_\" + n_gram_type + \"/\"\n",
        "  file_path = model_path + \"model\" + beam_name + \".pkl\"\n",
        "  joblib.dump(topic_model, file_path) \n",
        "\n",
        "\n",
        "# visualize barchart of topics\n",
        "def create_barcharts(topic_model):  \n",
        "  fig = topic_model.visualize_barchart(top_n_topics = len(topic_model.topics))\n",
        "  fig_name = model_path + \"bar_chart\" + beam_name  +\".html\"\n",
        "  fig_name_png = model_path + \"bar_chart\" + beam_name  +\".png\"\n",
        "  fig.write_html(fig_name)\n",
        "  fig.write_image(fig_name_png)\n",
        "  return fig\n",
        "\n",
        "# save topics in excel file\n",
        "def save_topic_results(topic_model, filename = \"Topic_Results.xlsx\"):\n",
        "  \n",
        "  excel_name = model_path + filename\n",
        "  df_topics = pd.DataFrame(topic_model.topics)\n",
        "  df_topics.to_excel(excel_name,sheet_name=\"topic_words\")\n",
        "\n",
        "\n",
        "def create_linecharts(topic_model, top_n_topics = 20):\n",
        "  # topic over time figures\n",
        "  fig_time = topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20) # save figure\n",
        "  fig_name_png = model_path + \"topic_time\" + beam_name  +\".png\"\n",
        "  fig_time.write_image(fig_name_png) # static image \n",
        "  fig_name_html = model_path + \"topic_time\" + beam_name  +\".html\"\n",
        "  fig_time.write_html(fig_name_html) # interactive html image\n",
        "  return fig_time\n",
        "\n",
        "def save_topic_sizes(topics):\n",
        "  counter = collections.Counter(topics)\n",
        "  df = pd.DataFrame(dict(counter).items())\n",
        "  df.columns = ['Topics', 'Size']\n",
        "  df.to_csv(model_path + 'topic_size' +'.csv')\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "n1Jl38iJ1Mhg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model_path) \n",
        "create_barcharts(topic_model)\n",
        "save_topic_results(topic_model)\n",
        "create_linecharts(topic_model)\n",
        "save_topic_sizes(topics)"
      ],
      "metadata": {
        "id": "_KhyDXyYB5v3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rec Engine"
      ],
      "metadata": {
        "id": "5Nm1F97rIdu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# topic embedding\n",
        "topic_emb = topic_model.topic_embeddings\n",
        "\n",
        "\n",
        "def create_sim_df(topic_num,doc_text):\n",
        "\n",
        "  '''\n",
        "  function to create a df with the representative dcouments and their similarity to the respective document \n",
        "\n",
        "\n",
        "  Input: \n",
        "  topic_num indicating which topic to calculate similarity from\n",
        "  doc_text: list of all docs within that topic\n",
        "\n",
        "  Output:\n",
        "  similatity dataframe with all the docs in one column and the similarity of the doc with the topic in the other (descending order)\n",
        "  '''\n",
        "\n",
        "\n",
        "  # document embeddings, depends on which embedding model used to train topic model\n",
        "  if model_use == \"specter\":\n",
        "    sentence_model = SentenceTransformer('allenai-specter')\n",
        "    doc_emb = sentence_model.encode(doc_text, show_progress_bar=False)\n",
        "  else:\n",
        "    sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    doc_emb = sentence_model.encode(doc_text, show_progress_bar=False)\n",
        "\n",
        "\n",
        "  sim_list = {}\n",
        "  for i in range(len(doc_text)):\n",
        "    sim_list[doc_text[i]]=(1-spatial.distance.cosine(doc_emb[i], topic_emb[topic_num])) # cosine similarity between all docs and one topic\n",
        "  \n",
        "  # sort dict in decreasing order \n",
        "  sim_list_sorted = dict(sorted(sim_list.items(), key=lambda item: item[1],reverse=True))\n",
        "\n",
        "  # make df and print head to get top 10 closest docs \n",
        "  import pandas as pd\n",
        "  sim_df = pd.DataFrame(sim_list_sorted.items(), columns=['Doc', 'Sim'])\n",
        "\n",
        "  return sim_df\n",
        "\n"
      ],
      "metadata": {
        "id": "_Prg9pNKuKmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###--------- for a given search term, find closest authors, documents-----------###\n",
        "search_term = input(\"Find authors and documents related to the term : \")\n",
        "\n",
        "\n",
        "# generating authors for each topic \n",
        "df_authors, dict_authors = author_all_topics(topics,input_data)\n",
        "# save df with authors \n",
        "excel_name = model_path + \"Author_Topics.xlsx\"\n",
        "df_authors.to_excel(excel_name,sheet_name=\"authors_topics\")\n",
        "\n",
        "# find most similar topics\n",
        "similar_topics, similarity = topic_model.find_topics(search_term, top_n=5)\n",
        "\n",
        "# print closest topic\n",
        "print(\"The closest topic is topic number {}: {}\".format(similar_topics[0],topic_model.topic_names[similar_topics[0]]))\n",
        "\n",
        "# find all authors in the most similar topic\n",
        "print(\"All authors who work in this space:\")\n",
        "author_similar = dict_authors[similar_topics[0]]\n",
        "print(author_similar)\n",
        "\n",
        "# get 3 rep docs related to a particular search term \n",
        "rep_docs = topic_model.get_representative_docs(similar_topics[0])\n",
        "print(\"Three Closest documents are:\") # this is using the get_representative_docs funciton from the package\n",
        "print(rep_docs)\n",
        "\n",
        "# get all docs related to that topic \n",
        "docs_topics = pd.DataFrame(topics, list_text).reset_index().rename(columns = {'index':'Document', 0:'Topic_Number'})\n",
        "\n",
        "# filter for the most similar topic\n",
        "docs_similar = docs_topics[docs_topics['Topic_Number']==similar_topics[0]]['Document']\n",
        "\n",
        "print(\"There are {} documents in this topic. They are (in order of similarity to topic):\".format(len(docs_similar)))\n",
        "\n",
        "create_sim_df(similar_topics[0]+1,docs_similar.to_list())\n",
        "\n"
      ],
      "metadata": {
        "id": "SJOvy6d3W8Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###----- for a given document, find closest authors and documents-----------###\n",
        "\n",
        "\n",
        "search_doc = \"Researchers have increased the lifetime of a promising electric vehicle \\\\\n",
        " battery to a record level, an important step toward the goal of lighter, less expensive and long-lasting batteries for future electric vehicles. The work is reported June 28 in the journal Nature Energy.Such batteries—the goal\\\\\n",
        "  of research groups the world over—are seen as an important part of the solution to reduce the effects of climate change, and scientists are exploring a dizzying array of options.\\\\\n",
        "One solution on the horizon is a lithium-metal battery for electric vehicles. These batteries hold almost twice the energy of their widely used lithium-ion counterparts, and they’re lighter.\\\\\n",
        " That combination offers the enticing prospect of an electric vehicle that would be lighter and go much farther on a single charge. But lithium-metal batteries in the laboratory have been plagued by premature death, lasting only a fraction of the time of today’s lithium-ion batteries.\"\n",
        "\n",
        "\n",
        "\n",
        "# find most similar topic\n",
        "similar_topics, similarity = topic_model.find_topics(search_doc, top_n=5)\n",
        "\n",
        "# print closest topic\n",
        "print(\"The closest topic is topic number {}: {}\".format(similar_topics[0],topic_model.topic_names[similar_topics[0]]))\n",
        "\n",
        "\n",
        "# find all authors in the most similar topic\n",
        "print(\"All authors who work in this space:\")\n",
        "author_similar = dict_authors[similar_topics[0]]\n",
        "print(author_similar)\n",
        "\n",
        "# get 3 rep docs related to a particular search term \n",
        "rep_docs = topic_model.get_representative_docs(similar_topics[0])\n",
        "print(\"Three Closest documents are:\")\n",
        "print(rep_docs)\n",
        "\n",
        "# get all docs related to that topic \n",
        "# filter for the most similar topic\n",
        "docs_similar = docs_topics[docs_topics['Topic_Number']==similar_topics[0]]['Document']\n",
        "\n",
        "print(\"There are {} documents in this topic. They are (in order of similarity to topic):\".format(len(docs_similar)))\n",
        "\n",
        "create_sim_df(similar_topics[0]+1,docs_similar.to_list())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U8Fd3Uuivgli",
        "outputId": "6148c108-0af6-4bc2-b3d6-2bc844450c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The closest topic is topic number 7: 7_lithium metal_block copolymer_current density_copolymer electrolyte\n",
            "All authors who work in this space:\n",
            "['Sulas, D.B.', 'Cheng, Lei', 'Platt, H.S.', 'Al-Jassim, M.M.', 'Ho, A.', 'Maslyn, Jacqueline', 'Yuca, N.', 'Frenck, Louise', 'Srinivasan, V.', 'Bird, O.F.', 'Renner, Peter', 'Wu, S.-L.', 'Parkinson, Dilworth Y.,McCloskey, Bryan', 'Sahu, S.', 'Parkinson, Dilworth Y.,Kundu, Shankhamala', 'Trask, S.E.', 'Chen, Yinlin', 'Meckler, S.M.', 'Jiang, X.', 'Ling, M.', 'Barai, P.', 'Yi, Eongyu', 'Mueller, K.T.', 'MacDowell, Alastair A.,Balsara, Nitash P.,', 'Amores, M.', 'Kim, J.', 'Tong, W.', 'Parkinson, Dilworth Y.,Liang, Hong', 'Pratt, R.C.', 'Müller, A.', 'Andrykowski, R.', 'Robbins, S.', 'Pylypenko, S.', 'Chen, Kelly', 'Doeff, Marca', 'Frenck, L.', 'Kim, H.W.', 'Marwaha, N.', 'Harry, Katherine', 'Baskin, A.', 'Heywood, S.', 'McEntush, Kyle', 'Zhang, Bixia', 'Viswanathan, V.', 'Teat, S.J.', 'Sankar, S.', 'Sung, Yung-Eun', 'Ells, A.W.', 'Seitzman, N.', 'Parkinson, D.Y.', 'Jansen, A.N.', 'Venturi, V.', 'Sofie, S.W.', 'Devaux, Didier H,Harry, Katherine', 'Mehta, S.', 'Schauser, N.S.', 'Higa, Kenneth', 'Battaglia, Vincent Santo,Parkinson, Dilworth Y.,Srinivasan, Venkat', 'Parkinson, Dilworth Y.,Balsara, Nitash P.,', 'Maslyn, J.A.', 'Finegan, D.P.', 'Devaux, Didier H,Wang, Xinru', 'Parkinson, Dilworth Y.,Minor, A.M.', 'Shen, Hao', 'Ma, Le', 'Song, J.', 'Cairns, Elton J.,', 'Srinivasan, Venkat', 'Parkinson, Dilworth Y.,Chen, Guangxu', 'Westmore, Tim', 'Pylypenko, Svitlana M,', 'Oh, H.J.', 'Liao, Xiaxia', 'Alvarado, Judith', 'Harry, K.J.', 'Jha, Swarn', 'Battaglia, V.', 'Loo, W.S.', 'Liu, Gang', 'Baird, M.A.', 'McEntush, K.D.', 'Prendergast, D.', 'Parkinson, Dilworth Y.,Yuan, Rodger', 'Sofie, S.', 'Liang, Hong', 'Eitouni, H.B.', 'Elwany, A.', 'Ahmad, Z.', 'Tamura, Nobumichi', 'Han, K.S.', 'Yang, Qin', 'Veeraraghavan, V.D.', 'Al-Jassim, M.', 'Balsara, N.P.', 'Parkinson, Dilworth Y.,MacDowell, Alastair A.,Balsara, Nitash P.,', 'Parkinson, Dilworth Y.,Watanabe, H.', 'Hwa, Yoon', 'Guthrey, H.', 'Zhao, Hongyu', 'Balsara, Nitash P.,', 'Villaluenga, I.', 'Carrington, M.E.', 'Ferreira, S.', 'Helms, B.A.', 'Kou, J.', 'Wang, Fan', 'Minor, A.M.', 'Ho, A.S.', 'Nuval, A.a.', 'Parkinson, Dilworth Y.,Doeff, Marca', 'Fu, C.', 'Thelen, Jacob L.,Parkinson, Dilworth Y.,Cabana, Jordi', 'Baran, M.J.', 'Chang, Y.H.', 'Parkinson, Dilworth Y.,Fu, Yang', 'Devaux, D.', 'Seitzman, Natalie', 'Hallinan, D.T.']\n",
            "Three Closest documents are:\n",
            "['Failure Mode Lithium Metal battery block Copolymer Electrolyte analyze x ray Microtomography Solid block polymer electrolyte promise candidate development high energy density rechargeable lithium metal base battery solid state battery comprise lithiummetal negative electrode lithium iron phosphate lifepo lt gt 4 lt gt composite positive electrode assemble polystyrene b poly ethylene oxide seo copolymer dope lithium salt use electrolyte cycling battery reason capacity fade failure determine imaging battery use synchrotron hard x ray microtomography experiment reveal partial delamination lithium foil block copolymer electrolyte layer void volume foil electrolyte layer obtain 40 90 cycle comparable volume change battery one cycle simple model account effect delamination current density battery present capacity fade battery failure observe experiment consistent model evidence lithium dendrite formation find contrast cycle lithium lithium symmetric cell polymer electrolyte current density fail due dendrite formation evidence delamination find cell', 'lithium Dendrite Growth Glassy Rubbery Nanostructured Block Copolymer Electrolytes enable use lithium metal anode critical step require dramatically increase energy density rechargeable battery however dendrite growth lithium metal battery lack fundamental understanding factor govern growth limit factor prevent adoption herein present effect battery cycling temperature range 90 120 ° c dendrite growth polystyrene block poly ethylene oxide -based electrolyte temperature range encompass glass transition temperature polystyrene 107 ° c slight increase cycling temperature symmetric lithium polymer lithium cell 90 105 ° c result factor five decrease amount charge pass short circuit synchrotron hard x ray microtomography experiment reveal shift dendrite location primarily within lithium electrode 90 ° c primarily within electrolyte 105 ° c rheological measurement show large change mechanical property temperature window time temperature superposition use interpret rheological datum Dendrite growth characteristic cell lifetime correlate temperature dependent shift factor use time temperature superposition work represent step toward understand factor govern lithium dendrite growth viscoelastic electrolyte', 'toward all solid State Lithium Batteries three dimensional visualization Lithium Migration β Li d3 PS d4 Ceramic Electrolyte all solid lithium battery attractive next generation technology use ion conduct solid β Li3PS4 LPS enable use lithium metal anode increase theoretical capacity widen stable voltage window traditional lithium ion system ion conductive solid also provide increase safety replace flammable liquid electrolyte although solid state electrolyte significantly stable dendrite resistant traditional liquid electrolyte lithium anode all solid system may nevertheless grow dendrite high stress repeat cycling lead short circuit premature battery breakdown reason study formation propagation Li metal feature within solid electrolyte use synchrotron base x ray tomography in situ current voltage cycling support custom sample platform result demonstrate ability technique delineate different layer Li LPS Li structure spatial resolution approach 1 μm resolution able detect expansion void especially early stage cycle expansion void observe throughout volume symmetric cell visually resemble propagation crack result interaction Li metal pre existing void LPS electrolyte']\n",
            "There are 34 documents in this topic. They are (in order of similarity to topic):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  Doc       Sim\n",
              "0   lithium Electrodeposition Rigid Block Copolyme...  0.558527\n",
              "1   lithium dendrite growth solid polymer electrol...  0.542074\n",
              "2   lithium Dendrite Growth Glassy Rubbery Nanostr...  0.532473\n",
              "3   lithium Metal copper Vanadium Oxide Battery Bl...  0.525009\n",
              "4   growth Lithium Dendrites Globules Solid Block ...  0.518955\n",
              "5   Influence Electrolyte Modulus Local Current De...  0.509452\n",
              "6   Failure Mode Lithium Metal battery block Copol...  0.502882\n",
              "7   detection subsurface structure underneath dend...  0.502662\n",
              "8   Electrochemical Deposition Stripping Behavior ...  0.500630\n",
              "9       lithium metal foil low defect density pending  0.494996\n",
              "10  lithium Sulfur Batteries Block Copolymer Elect...  0.484024\n",
              "11  multi scale microscopy Li Electrolyte Interfac...  0.481605\n",
              "12  Impact Salt Concentration Nonuniform Lithium E...  0.477661\n",
              "13  preferential strip Lithium Protrusion Resultin...  0.474317\n",
              "14  uncover relationship Diameter Height electrode...  0.470488\n",
              "15  3D Detection Lithiation Lithium Plating Graphi...  0.464888\n",
              "16  limit Current Nanostructured Block Copolymer E...  0.455802\n",
              "17  study buffer layer base block copolymer electr...  0.455611\n",
              "18  toward all solid State Lithium Batteries three...  0.454338\n",
              "19  Operando x ray Tomography Imaging Solid State ...  0.449769\n",
              "20  failure Analysis Batteries use Synchrotron bas...  0.426842\n",
              "21  All solid State Batteries use rationally desig...  0.402201\n",
              "22  Convenient Versatile Method Control Electrode ...  0.400094\n",
              "23  Extended Cycling Rigid Block Copolymer Electro...  0.399519\n",
              "24  Universal chemomechanical design rule solid io...  0.394530\n",
              "25  diversity orient synthesis polymer membrane io...  0.344300\n",
              "26  orient porous llzo 3d structure obtain freeze ...  0.338195\n",
              "27  NiWO4 Nanoparticle decorate Lignin Electrodes ...  0.317176\n",
              "28  scalable Freeze tape cast Fabrication Pore Str...  0.313442\n",
              "29  influence morphology electrochemical capacity ...  0.299720\n",
              "30  sustainable sulfur carbonaceous composite elec...  0.295588\n",
              "31  compare Macroscale Microscale Simulations Poro...  0.276558\n",
              "32  Design Synthesis Lignin base Flexible Supercap...  0.199494\n",
              "33  three dimensionally align Sulfur Electrodes Di...  0.183377"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3dd08f64-88ae-48dc-b98a-23b0646fb3f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc</th>\n",
              "      <th>Sim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lithium Electrodeposition Rigid Block Copolyme...</td>\n",
              "      <td>0.558527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lithium dendrite growth solid polymer electrol...</td>\n",
              "      <td>0.542074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lithium Dendrite Growth Glassy Rubbery Nanostr...</td>\n",
              "      <td>0.532473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lithium Metal copper Vanadium Oxide Battery Bl...</td>\n",
              "      <td>0.525009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>growth Lithium Dendrites Globules Solid Block ...</td>\n",
              "      <td>0.518955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Influence Electrolyte Modulus Local Current De...</td>\n",
              "      <td>0.509452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Failure Mode Lithium Metal battery block Copol...</td>\n",
              "      <td>0.502882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>detection subsurface structure underneath dend...</td>\n",
              "      <td>0.502662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Electrochemical Deposition Stripping Behavior ...</td>\n",
              "      <td>0.500630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>lithium metal foil low defect density pending</td>\n",
              "      <td>0.494996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>lithium Sulfur Batteries Block Copolymer Elect...</td>\n",
              "      <td>0.484024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>multi scale microscopy Li Electrolyte Interfac...</td>\n",
              "      <td>0.481605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Impact Salt Concentration Nonuniform Lithium E...</td>\n",
              "      <td>0.477661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>preferential strip Lithium Protrusion Resultin...</td>\n",
              "      <td>0.474317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>uncover relationship Diameter Height electrode...</td>\n",
              "      <td>0.470488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3D Detection Lithiation Lithium Plating Graphi...</td>\n",
              "      <td>0.464888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>limit Current Nanostructured Block Copolymer E...</td>\n",
              "      <td>0.455802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>study buffer layer base block copolymer electr...</td>\n",
              "      <td>0.455611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>toward all solid State Lithium Batteries three...</td>\n",
              "      <td>0.454338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Operando x ray Tomography Imaging Solid State ...</td>\n",
              "      <td>0.449769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>failure Analysis Batteries use Synchrotron bas...</td>\n",
              "      <td>0.426842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>All solid State Batteries use rationally desig...</td>\n",
              "      <td>0.402201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Convenient Versatile Method Control Electrode ...</td>\n",
              "      <td>0.400094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Extended Cycling Rigid Block Copolymer Electro...</td>\n",
              "      <td>0.399519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Universal chemomechanical design rule solid io...</td>\n",
              "      <td>0.394530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>diversity orient synthesis polymer membrane io...</td>\n",
              "      <td>0.344300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>orient porous llzo 3d structure obtain freeze ...</td>\n",
              "      <td>0.338195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NiWO4 Nanoparticle decorate Lignin Electrodes ...</td>\n",
              "      <td>0.317176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>scalable Freeze tape cast Fabrication Pore Str...</td>\n",
              "      <td>0.313442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>influence morphology electrochemical capacity ...</td>\n",
              "      <td>0.299720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>sustainable sulfur carbonaceous composite elec...</td>\n",
              "      <td>0.295588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>compare Macroscale Microscale Simulations Poro...</td>\n",
              "      <td>0.276558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Design Synthesis Lignin base Flexible Supercap...</td>\n",
              "      <td>0.199494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>three dimensionally align Sulfur Electrodes Di...</td>\n",
              "      <td>0.183377</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dd08f64-88ae-48dc-b98a-23b0646fb3f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dd08f64-88ae-48dc-b98a-23b0646fb3f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dd08f64-88ae-48dc-b98a-23b0646fb3f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###----- for a given auhors, find all other authors and closest documents-----------###\n",
        "\n",
        "# iterate thru dict, pick up the topic where that author occurs and return all authors in that topic\n",
        "# search_author = 'Balsara, N.P.'\n",
        "search_author = 'Zenyuk, I.V.'\n",
        "\n",
        "for t in dict_authors:\n",
        "  if search_author in dict_authors[t]:\n",
        "    # print closest topic\n",
        "    print(\"The given author works in Topic Number {}: {}\".format(t,topic_model.topic_names[t]))\n",
        "    first_auth_list = dict_authors[t]\n",
        "    # print(\"All other authors:\",dict_authors[t])\n",
        "\n",
        "\n",
        "# find the doc in which the search author occurs, then find closest docs to that doc and return authors for that\n",
        "relevant_docs = input_data[input_data.apply(lambda row: row.astype(str).str.contains(search_author).any(), axis=1)]['combined'].to_list()\n",
        "authors_docs = closest_author_docs(relevant_docs)\n",
        "# print(\"Authors representing the closest docs are:\",authors_docs)\n",
        "\n",
        "final_auth_list = authors_docs + first_auth_list\n",
        "final_auth_list = list(set(final_auth_list))\n",
        "print(\"Closest authors to given author:\",final_auth_list)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6qTKoSr5WTO",
        "outputId": "91b97dbb-ea44-4b99-d082-0c9b3956f188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The given author works in Topic Number 4: 4_fuel cell_polymer electrolyte_liquid water_gas diffusion\n",
            "Closest authors to given author: ['Spernjak, D.', 'Chan, T.', 'Pan, X.', 'Karan, K.', 'Sabarirajan, Dinesh C,Calzada, O.', 'Mukundan, R.', 'Dery, S.', 'Garcia-Salaberri, P.A.', 'Vera, M.', 'Ng, B.', 'Besli, M.M.', 'McElrone, Andrew Joseph,Choat, Brendan', 'Martin, Andreas', 'Parkinson, Dilworth Y.,Kusoglu, Ahmet', 'Tucker, M.C.', 'Sabarirajan, Dinesh C,Yared, D.G.', 'Parkinson, Dilworth Y.,Serov, A.', 'Gili, A.', 'Shum, A.D.', 'Connolly, L.G.', 'Parkinson, Dilworth Y.,Blair, J.', 'Cetinbas, F.C.', 'Gambetta, Gregory A.,Brodersen, Craig', 'Tieu, P.', 'Fendorf, Scott E.,Luthy, Richard G,', 'Serov, A.', 'Normile, S.J.', 'Weidner, J.W.', 'Medici, E.F.', 'Gostick, Jeff T.,Gunterman, Haluna Penelope F.,Kienitz, Brian L.,Newman, James', 'Khedekar, K.', 'Weber, Adam Z,Gostick, Jeff T.,', 'Mansour, Nagi N,', 'Zheng, Y.', 'Gross, Elad', 'Xiao, X.', 'Varcoe, J.R.', 'Weber, Adam Z,Kusoglu, Ahmet', 'García-Salaberri, P.A.', 'MacDowell, Alastair A.,Weber, Adam Z,', 'More, K.L.', 'Barnard, H.S.', 'Kusoglu, Ahmet', 'Cheng, Y.', 'Lu, Zijie', 'Hexemer, Alexander', 'Shum, Andrew D,Zenyuk, Iryna V,Shimpalee, S.', 'Perego, A.', 'Shum, Andrew D,Zenyuk, Iryna V,', 'Zenyuk, Iryna V,Parkinson, Dilworth Y.,Weber, Adam Z,Allen, J.S.', 'Hussey, D.S.', 'Liu, Yao', 'Hu, B.', 'Johnston, C.M.', 'Atanassov, Plamen', 'Tamura, N.', 'Kim, D.W.', 'Shum, Andrew D,Normile, S.', 'Weber, A.Z.', 'Shimpalee, S.', 'Pant, L.M.', 'Shen, F.', 'De Carlo, F.', 'Borup, R.L.', 'Ahluwalia, R.K.', 'Satjaritanun, P.', 'Parkinson, D.Y.', 'Weber, Adam Z,', 'Panerai, Francesco', 'Danilovic, N.', 'Roy, A.', 'Omasta, T.J.', 'Leonard, E.', 'Borner, Arnaud', 'Ayers, K.E.', 'Zenyuk, Iryna V,Parkinson, Dilworth Y.,Hwang, G.k.', 'Higa, Kenneth', 'Zenyuk, I.V.', 'Cheng, L.', 'LaManna, J.M.', 'Talarposhti, M.R.', 'Zenyuk, Iryna V,Weber, Adam Z,', 'Gasch, M.J.', 'Artyushkova, K.', 'Wang, L.', 'Zenyuk, Iryna V,', 'Goulas, Konstantinos', 'Jambunathan, R.', 'Khunatorn, Y.', 'Villatoro, W.', 'Srinivasan, Venkat', 'Li, Menglu', 'Ahn, Suk Jin', 'Zackin, B.I.', 'Huang, Y.', 'Ferguson, J.', 'Rezaei Talarposhti, M.', 'Litster, S.E.', 'Hirano, S.', 'Zhuang, Y.', 'Tu, K.N.', 'Doran, A.', 'Liu, Gang', 'Xiao, Xiao', 'Craig, N.', 'Asset, T.', 'Wang, R.', 'Parkinson, Dilworth Y.,Barnard, Harold S,Ling, M.', 'Leonard, Emily', 'Zenyuk, Iryna V,Shum, Andrew D,Hwang, Gi Suk', 'Hickner, Michael A,', 'Kulkarni, D.', 'Lachaud, J.', 'Tippayawong, N.', 'Levin, D.A.', 'Zhao, Hongyu', 'Capuano, C.', 'Xu, M.', 'Kuppan, S.', 'Mustain, W.E.', 'Hwang, Gi Suk', 'Ogawa, S.', 'Stewart, S.', 'Peng, X.', 'Sepe, M.', 'Ferguson, Joseph C.,Lachaud, J.', 'Weber, Adam Z,Zenyuk, Iryna V,', 'Atanassov, P.', 'Parkinson, Dilworth Y.,Jacobson, D.L.', 'Chen, Y.', 'Liu, J.', 'Metzger, M.', 'Mandal, Pratiti', 'Wojcik, M.', 'Zenyuk, Iryna V,Hwang, Gi Suk', 'Ferguson, Joseph C.,Panerai, Francesco', 'De Andrade, V.', 'Dietrich, Philipp', 'Johnson, Gregory R,Grippo, Adam Michael,Wang, Y.C.', 'Gu, S.', 'Dogdibegovic, E.', 'Seyfferth, Angelia L.,Masue-Slowey, Y.', 'Hwang, G.S.', 'Gostick, Jeff T.,Hwang, Gi Suk', \"O'Brien, M.\", 'Sabarirajan, D.C.', 'Weber, Adam Z,Vera, M.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7bOgaKl7GOjA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
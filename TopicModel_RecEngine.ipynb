{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModel_RecEngine.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNs/Nl4Mc5To7xIYuvbQ19B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dani-lbnl/mudit/blob/main/TopicModel_RecEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install and imports"
      ],
      "metadata": {
        "id": "OX_a3v9tWeSh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "26N2QnWaUQI7",
        "outputId": "e02e3e60-4061-482d-cba0-05d750fe7369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 66.2 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 35.7 MB/s \n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 76.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.16 typer-0.4.1 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.3.0/en_core_web_lg-3.3.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 400.7 MB 7.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (8.0.16)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9 MB 156 kB/s \n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.10.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting sentence-transformers>=0.4.1\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml<6.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (3.13)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.21.6)\n",
            "Collecting hdbscan>=0.8.28\n",
            "  Downloading hdbscan-0.8.28.tar.gz (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 45.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.7/dist-packages (from bertopic) (5.5.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from bertopic) (1.0.2)\n",
            "Collecting umap-learn>=0.5.0\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (1.4.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.28->bertopic) (0.29.30)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.12.0+cu113)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml<6.0\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.9)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.0->bertopic) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.34.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-linux_x86_64.whl size=2342158 sha256=4e137242b0a83dd564baffdee741a597ca2fcb27d744145ffab5588d564232b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/7a/5e/259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=d548bdba363e8a302fd326ae3793417d7855570c4fa7c522702c567c299aed4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=3bb4ec48c3f85ab5e61c5ba96b21a711651ce7ad18986a77b23a2a3f03e5e587\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=e8112333222585bcf6fcd186b384733476a2e85d5f3d4fd083eb23a7ea103f24\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentencepiece, pynndescent, umap-learn, sentence-transformers, hdbscan, bertopic\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed bertopic-0.10.0 hdbscan-0.8.28 huggingface-hub-0.6.0 pynndescent-0.5.7 pyyaml-5.4.1 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.19.2 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install -U kaleido\n",
        "!pip install bertopic\n",
        "# !python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from top2vec import Top2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import time\n",
        "from google.colab import drive\n",
        "import os\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import kaleido\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "multiprocessing.cpu_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNNNPo-sWl-W",
        "outputId": "b4c61e25-2363-494e-8d2a-f969b962e220"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## helper functions"
      ],
      "metadata": {
        "id": "4AbiC8mxWsqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os \n",
        "\n",
        "def create_path_if_not_exists(datapath):\n",
        "    '''Create the new file if not exists and save the data'''\n",
        "\n",
        "    if not os.path.exists(datapath):\n",
        "        os.makedirs(datapath) \n",
        "\n",
        "def remove_punct_df(df):\n",
        "\n",
        "  # remove punctuation\n",
        "  # text_col = [''.join(letter for letter in word if letter not in string.punctuation) for word in df]\n",
        "\n",
        "  filt_combined = []\n",
        "  for word in word_tokenize(df):\n",
        "\n",
        "    if word.lower() not in string.punctuation:\n",
        "      # print(word)\n",
        "      \n",
        "      filt_combined.append(word)\n",
        "  filtered_ip= \" \".join(filt_combined)\n",
        "\n",
        "  return filtered_ip\n",
        "\n",
        "def create_input_new(df):\n",
        "\n",
        "  # for title\n",
        "  indices = df['Title'].isna()\n",
        "  df.loc[indices,'Title'] = \"\"\n",
        "\n",
        "  # for abstract\n",
        "  indices = df['Abstract'].isna()\n",
        "  df.loc[indices,'Abstract'] = \"\"\n",
        "\n",
        "  # combined - title + abstract\n",
        "  df['Combined'] = df['Title'] + \" \" + df['Abstract']\n",
        "\n",
        "  # remove blanks\n",
        "  df = df[df['Combined']!=\" \"]\n",
        "\n",
        "  #keep only selected cols\n",
        "  df_sel = df[['Authors','Pub Year','Research Area','Combined',\"Pub TYpe\"]]\n",
        "  df_sel= df_sel.rename(columns={'Pub Year':'pub_year',\"Research Area\":\"research_area\",\"Authors\":\"authors\",\"Combined\":\"combined\",\"Pub TYpe\":\"pub_type\"})\n",
        "\n",
        "  combined = list(df_sel['combined'])\n",
        "\n",
        "  # remove patterns\n",
        "  pattern = r'<inf>|</inf>|<sup>|</sup>|inf|/inf'\n",
        "  comb_clean = []\n",
        "  for l in combined:\n",
        "    mod_string = re.sub(pattern, '', l )\n",
        "    comb_clean.append(mod_string)\n",
        "\n",
        "  # merge back to df\n",
        "  df_sel['combined'] = comb_clean\n",
        "\n",
        "  # filter spurtious yeats\n",
        "  df_sel = df_sel[df_sel['pub_year']!='12.0.1.2']\n",
        "\n",
        "  # convert years to int\n",
        "  # df_sel['pub_year'] = df_sel['pub_year'].astype(str)\n",
        "  df_sel['pub_year'] = df_sel['pub_year'].astype(str).replace('\\.0', '', regex=True).astype(int)\n",
        "  # if year is 201, that is mistyped fom 2001\n",
        "  df_sel[df_sel['pub_year']==201]['pub_year'] = 2001\n",
        "\n",
        "\n",
        "  return df_sel\n",
        "\n",
        "def lemma_spacy(df_combined):\n",
        "\n",
        "  filt_combined = []\n",
        "  for word in nlp(df_combined):\n",
        "    if word.lemma_ != '-PRON-' :\n",
        "      filt_combined.append(word.lemma_)\n",
        "\n",
        "  new_df = \" \".join(filt_combined)\n",
        "\n",
        "  return new_df  \n",
        "\n",
        "def remove_stop_df(df_combined):\n",
        "\n",
        "  # remove stopwords\n",
        "\n",
        "  new_df= []\n",
        "  filt_combined = []\n",
        "  for word in word_tokenize(df_combined):\n",
        "\n",
        "    if word.lower() not in stopwords.words('english'):\n",
        "      # print(word)\n",
        "      if word.lower() == \"perovskites\":\n",
        "        filt_combined.append(\"perovskite\")\n",
        "      else:\n",
        "        filt_combined.append(word)\n",
        "  filtered_ip= \" \".join(filt_combined)\n",
        "\n",
        "  return filtered_ip\n",
        "\n",
        "\n",
        "\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "\n",
        "def get_authors(input_data,rep_docs):\n",
        "  tt = input_data['combined'].to_list()\n",
        "\n",
        "  auth_list= []\n",
        "  for d in rep_docs:\n",
        "    ind = tt.index(d)\n",
        "    auth_str = input_data.authors[ind]\n",
        "    auth_el = auth_str.split(\" ,\")\n",
        "    auth_list.append(auth_el)\n",
        "\n",
        "  auth_list = flatten(auth_list)\n",
        "  if '' in auth_list:\n",
        "    auth_list.remove('')\n",
        "\n",
        "  # unduplicate repeating authors \n",
        "  # print(\"Length of List:\",len(auth_list))\n",
        "  # print(auth_list)\n",
        "\n",
        "\n",
        "  final_auth_list = list(set(auth_list))\n",
        "  if '' in final_auth_list:\n",
        "    final_auth_list.remove('')\n",
        "\n",
        "  # print(final_auth_list)\n",
        "  # print(\"Length of Final List:\",len(final_auth_list))\n",
        "\n",
        "  return final_auth_list\n",
        "\n",
        "  \n",
        "def author_all_topics(model,input_data):\n",
        "\n",
        "  dict_df = {}\n",
        "  for i in range(len(model.topics)-1):\n",
        "    rep_docs = model.get_representative_docs(i)\n",
        "    author_list = get_authors(input_data,rep_docs)\n",
        "\n",
        "    dict_df[i] = author_list\n",
        "\n",
        "  # create df with topics and authors\n",
        "  author_topics = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in dict_df.items() ]))\n",
        "\n",
        "  return author_topics,dict_df\n",
        "\n"
      ],
      "metadata": {
        "id": "WKEYleUdWu4Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## path of files "
      ],
      "metadata": {
        "id": "Pom9Fj3nWxWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "# read all files\n",
        "path = '/content/drive/MyDrive/NLP/ALS Spreadsheets/'\n",
        "files = os.listdir(path)\n",
        "files.sort()\n",
        "files\n",
        "# beamline 2.4 bad ..removed; also one more check which one\n",
        "# 3.2.1 bad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg_FCIh-Wz4A",
        "outputId": "303d05b2-20f7-4432-d82c-d2f77e0be835"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beamline_1.4.xls',\n",
              " 'Beamline_10.0.1.xls',\n",
              " 'Beamline_10.3.1.xls',\n",
              " 'Beamline_10.3.2.xls',\n",
              " 'Beamline_11.0.1.xls',\n",
              " 'Beamline_11.0.2.xls',\n",
              " 'Beamline_11.3.1.xls',\n",
              " 'Beamline_11.3.2.xls',\n",
              " 'Beamline_12.0.1.1.xls',\n",
              " 'Beamline_12.0.1.2.xls',\n",
              " 'Beamline_12.0.1.4.xls',\n",
              " 'Beamline_12.0.2.xls',\n",
              " 'Beamline_12.2.1.xls',\n",
              " 'Beamline_12.2.2.xls',\n",
              " 'Beamline_12.3.1.xls',\n",
              " 'Beamline_12.3.2.xls',\n",
              " 'Beamline_2.1.xls',\n",
              " 'Beamline_3.1.1.xls',\n",
              " 'Beamline_3.3.2.xls',\n",
              " 'Beamline_4.0.2.xls',\n",
              " 'Beamline_4.0.3.1.xls',\n",
              " 'Beamline_4.0.3.2.xls',\n",
              " 'Beamline_4.2.2.xls',\n",
              " 'Beamline_5.0.1.xls',\n",
              " 'Beamline_5.0.2.xls',\n",
              " 'Beamline_5.0.3.xls',\n",
              " 'Beamline_5.3.1.xls',\n",
              " 'Beamline_5.3.2.1.xls',\n",
              " 'Beamline_5.3.2.2.xls',\n",
              " 'Beamline_5.4.xls',\n",
              " 'Beamline_6.0.1.xls',\n",
              " 'Beamline_6.0.2.xls',\n",
              " 'Beamline_6.1.2.xls',\n",
              " 'Beamline_6.3.1.xls',\n",
              " 'Beamline_6.3.2.xls',\n",
              " 'Beamline_7.0.1.xls',\n",
              " 'Beamline_7.0.2.xls',\n",
              " 'Beamline_7.3.1.xls',\n",
              " 'Beamline_7.3.3.xls',\n",
              " 'Beamline_8.0.1.xls',\n",
              " 'Beamline_8.2.1.xls',\n",
              " 'Beamline_8.2.2.xls',\n",
              " 'Beamline_8.3.1.xls',\n",
              " 'Beamline_8.3.2.xls',\n",
              " 'Beamline_9.0.xls',\n",
              " 'Beamline_9.3.1.xls',\n",
              " 'beamline_4.0.3.xls']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main code"
      ],
      "metadata": {
        "id": "zEkbronsW3NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# which model to use - \"default\" or \"specter\"\n",
        "model_use = \"default\"\n",
        "\n",
        "# which beamline?\n",
        "beamline = \"8.3.2\"\n",
        "\n"
      ],
      "metadata": {
        "id": "LweRYa_UXVFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "iter_version = \"BERTopic_SPECTER_v3/\"\n",
        "\n",
        "for i in range(1):\n",
        "# for i in range(len(files)):\n",
        "  print(\"File:\",str(i+1) + ' ' + files[i])\n",
        "  # beam_name = files[i].split('xls')[0][:-1]\n",
        "  beam_name = \"Beamline_8.3.2.xls\"\n",
        "  \n",
        "\n",
        "  df = pd.read_table(path + beam_name, on_bad_lines='skip')\n",
        "  # df = pd.read_table(path + files[i], on_bad_lines='skip')\n",
        "\n",
        "\n",
        "  # combine abstract and title\n",
        "  input_data = create_input_new(df)\n",
        "\n",
        "  # remove stopwords\n",
        "  input_data['combined'] = input_data['combined'].apply(remove_stop_df)\n",
        "\n",
        "  # lemmatize\n",
        "  input_data['combined'] = input_data['combined'].apply(lemma_spacy)\n",
        "\n",
        "  # remove punct\n",
        "  input_data['combined'] = input_data['combined'].apply(remove_punct_df)\n",
        "\n",
        "  # train model\n",
        "  sentence_model = SentenceTransformer('allenai-specter')\n",
        "  topic_model_specter = BERTopic(embedding_model=sentence_model,verbose=True)\n",
        "  list_text = input_data['combined'].to_list()\n",
        "\n",
        "  topics, probs = topic_model_specter.fit_transform(list_text)\n",
        "  # print(\"Before removing:\")\n",
        "  # print(topic_model_specter.topics)\n",
        "\n",
        "  # topic_model_specter.topics.pop(-1, None)\n",
        "  # print(\"After removing\")\n",
        "  # print(topic_model_specter.topics)\n",
        "\n",
        "  # defining path\n",
        "  base_path = '/content/drive/MyDrive/NLP/'\n",
        "  model_path = base_path + iter_version + beam_name + \"/\"\n",
        "  create_path_if_not_exists(model_path)\n",
        "\n",
        "  # save model as pickle file\n",
        "  file_path = model_path + \"model\" + beam_name + \".pkl\"\n",
        "  joblib.dump(topic_model_specter, file_path) \n",
        "\n",
        "\n",
        "  # dynamic topic modeling\n",
        "  years = input_data['pub_year'].to_list()\n",
        "  topics_over_time = topic_model_specter.topics_over_time(list_text, topics, years)\n",
        "  fig_time = topic_model_specter.visualize_topics_over_time(topics_over_time, top_n_topics=20)\n",
        "  fig_name_png = model_path + \"topic_time\" + beam_name  +\".png\"\n",
        "  fig_time.write_image(fig_name_png)\n",
        "  fig_name_html = model_path + \"topic_time\" + beam_name  +\".html\"\n",
        "  fig_time.write_html(fig_name_html)\n",
        "\n",
        "  # visualize barchart of topics as html\n",
        "  fig = topic_model_specter.visualize_barchart(top_n_topics = len(topic_model_specter.topics))\n",
        "  fig_name = model_path + \"bar_chart\" + beam_name  +\".html\"\n",
        "  fig_name_png = model_path + \"bar_chart\" + beam_name  +\".png\"\n",
        "\n",
        "  fig.write_html(fig_name)\n",
        "  fig.write_image(fig_name_png)\n",
        "\n",
        "\n",
        "  # save as excel\n",
        "  # df_info = pd.DataFrame(model.get_topic_info())\n",
        "  excel_name = model_path + \"Topic_Results.xlsx\"\n",
        "  # df_info.to_excel(excel_name,sheet_name=\"topic_info\")\n",
        "\n",
        "  df_topics = pd.DataFrame(topic_model_specter.topics)\n",
        "  df_topics.to_excel(excel_name,sheet_name=\"topic_words\")\n",
        "\n",
        "\n",
        "  ## generating authors for each topic \n",
        "  df_authors, dict_authors = author_all_topics(topic_model_specter,input_data)\n",
        "  # save df with authors \n",
        "  excel_name = model_path + \"Author_Topics.xlsx\"\n",
        "  df_authors.to_excel(excel_name,sheet_name=\"authors_topics\")\n",
        "\n",
        "  search_term = input(\"Find authors and documents related to the term : \")\n",
        "\n",
        "\n",
        "  similar_topics, similarity = topic_model_specter.find_topics(search_term, top_n=5)\n",
        "  print(\"Authors are:\",dict_authors[similar_topics[0]])\n",
        "\n",
        "  # get all docs related to a particular search term \n",
        "  rep_docs = topic_model_specter.get_representative_docs(similar_topics[0])\n",
        "  print(\"Closest documents are:\",rep_docs)\n",
        "\n",
        "  \n",
        "  print(\"File: {} done \".format(i+1) )\n",
        "\n",
        "\n",
        "print('Total time taken (mins): ', int((time.time()-start_time)/60))\n",
        "\n"
      ],
      "metadata": {
        "id": "SJOvy6d3W8Rk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}